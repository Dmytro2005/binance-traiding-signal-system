{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a final version of the model training code markdown file. Print statement are added for tracking where execution is at atm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with imports, self-expanatory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.functions import (\n",
    "    col, lag, lead, when, unix_timestamp, floor, lit,\n",
    "    first, last, max as _max, min as _min, sum as _sum, input_file_name, regexp_extract,\n",
    "    pow, sqrt, udf, avg, abs as sabs, greatest\n",
    ")\n",
    "from functools import reduce\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier, GBTClassifier, LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "import os\n",
    "import sys\n",
    "import psutil\n",
    "import gc\n",
    "import re\n",
    "import shutil\n",
    "import time\n",
    "import tempfile\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates a UDF that extracts a probability of UP from Spark ML. Spark outputs predictions with probability vectors, this accesses the second element which is UP. We'll need this for later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_prob_udf = udf(lambda v: float(v[1]) if v is not None else 0.5, DoubleType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memory monitoring function to keep track of RAM and swap file memory use, added since the model was initially training on 16GB RAM laptop (probably still is, but might be changed to a cluster of 32GB later if i'm not lazy). 1e9 converts bytes to gigabytes, .1f formats to 1 decimal point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_memory():\n",
    "    mem = psutil.virtual_memory()\n",
    "    swap = psutil.swap_memory()\n",
    "    print(f\"Memory: {mem.percent}% used ({mem.used/1e9:.1f}GB/{mem.total/1e9:.1f}GB)\")\n",
    "    if swap.percent > 5:\n",
    "        print(f\"Swap: {swap.percent}% used ({swap.used/1e9:.1f}GB/{swap.total/1e9:.1f}GB) \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section configures the PySpark environment and sets up the necessary directory structure for the project. Sets python executability paths, hadoop home directory creates dirs for Spark operations, model storage and data processing. Old checkpoints and temporary files are cleaned to prevent conflicts from previous runs. A bunch of this section is hardcoded and taylored to a specific local machine the training was ran on due to troubles with windows related errors and version conflicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "hadoop_home = os.environ.get('HADOOP_HOME', r'C:\\hadoop')\n",
    "os.environ['HADOOP_HOME'] = hadoop_home\n",
    "BASE_TEMP_DIR = Path(tempfile.gettempdir()) / \"spark_crypto_ml\"\n",
    "SPARK_LOCAL_DIRS = str(BASE_TEMP_DIR / \"temp\")\n",
    "try:\n",
    "    BASE_DIR = Path(__file__).resolve().parent.parent\n",
    "except NameError:\n",
    "    BASE_DIR = Path(os.getcwd()).resolve().parent\n",
    "DATA_FOLDER =(os.path.join(BASE_DIR, \"data\", \"raw\"))\n",
    "MODEL_FOLDER = os.path.join(BASE_DIR, \"models\")\n",
    "CHECKPOINT_DIR = os.path.join(BASE_DIR, \"checkpoints\")\n",
    "\n",
    "if os.path.exists(CHECKPOINT_DIR):\n",
    "    shutil.rmtree(CHECKPOINT_DIR)\n",
    "    print(f\"Old checkpoints removed\")\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "#cleaning up old temp dir\n",
    "try:\n",
    "    if os.path.exists(str(BASE_TEMP_DIR)):\n",
    "        shutil.rmtree(str(BASE_TEMP_DIR))\n",
    "        print(f\"Temp directory cleaned: {BASE_TEMP_DIR}\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not clean temp directory - {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark setup. Creates and configures the Spark session with optimized settings for machine learning workloads. Key configurations include adaptive query execution, increased memory allocation (6GB driver, 9GB executor), Kryo serialization for performance, compression for RDDs and shuffles, and disabled code generation to avoid Windows compatibility issues. The session is set to ERROR-level logging and uses a dedicated checkpoint directory for fault tolerance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession.builder\n",
    "    .appName(\"Stacking_Ensemble_Training\")\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"32\")\n",
    "    .config(\"spark.default.parallelism\", \"32\")\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "    .config(\"spark.driver.memory\", \"6g\")\n",
    "    .config(\"spark.executor.memory\", \"9g\")\n",
    "    .config(\"spark.driver.maxResultSize\", \"3g\")\n",
    "    .config(\"spark.memory.fraction\", \"0.8\")\n",
    "    .config(\"spark.memory.storageFraction\", \"0.3\")\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "    .config(\"spark.rdd.compress\", \"true\")\n",
    "    .config(\"spark.shuffle.compress\", \"true\")\n",
    "    .config(\"spark.shuffle.spill.compress\", \"true\")\n",
    "    .config(\"spark.ui.enabled\", \"true\")\n",
    "    .config(\"spark.ui.port\", \"4040\")\n",
    "    .config(\"spark.sql.codegen.wholeStage\", \"false\")\n",
    "    .config(\"spark.sql.codegen.factoryMode\", \"NO_CODEGEN\")\n",
    "    .config(\"hadoop.native.lib\", \"false\")\n",
    "    .config(\"spark.hadoop.fs.file.impl\", \"org.apache.hadoop.fs.RawLocalFileSystem\")\n",
    "    .getOrCreate())\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "spark.sparkContext.setCheckpointDir(CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration, LOOKBACK and LAG_INTERVALS are used to set up how many previous candles are used for feature generation. CANDLE_MINUTES sets up a variable for aggregating 1m candles from the dataset into larger timeframes. ENABLE_GBT_UNDERSAMPLING is a flag used to enable or disable the undersampling of gradient boosted trees during training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LOOKBACK = 20\n",
    "LAG_INTERVALS = [i+1 for i in range(LOOKBACK)]\n",
    "CANDLE_MINUTES = 30\n",
    "ENABLE_GBT_UNDERSAMPLING = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading parquet files from the data folder containing 1-minute OHLCV candlestick data for multiple cryptocurrencies. Files are sorted and split chronologically into 80% training and 20% testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_parquet_files = sorted([f for f in os.listdir(DATA_FOLDER) if f.endswith(\".parquet\")])\n",
    "split_idx = int(len(all_parquet_files) * 0.8)\n",
    "train_files = all_parquet_files[:split_idx]\n",
    "test_files = all_parquet_files[split_idx:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracts cryptocurrency symbols from filenames using regex pattern matching to create a symbol column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'BTCDOWN-USDT', 'YFI-BUSD', 'MASK-USDT', 'SAND-BNB', 'AST-BTC', 'PSG-BTC', 'MITH-BNB', 'EOS-EUR', 'BNT-ETH', 'HBAR-BNB', 'LTC-BTC', 'C98-USDT', 'IOTA-BNB', 'MTL-ETH', 'QKC-BTC', 'TRX-TRY', 'ZRX-ETH', 'BCH-EUR', 'EGLD-BUSD', 'IOTX-USDT', 'LRC-BUSD', 'LAZIO-USDT', 'OST-ETH', 'LOOM-BTC', 'KAVA-BTC', 'SALT-BTC', 'ROSE-BUSD', 'TORN-USDT', 'DOT-BRL', 'XRPUP-USDT', 'XLM-ETH', 'DASH-ETH', 'EOS-USDC', 'EOS-TUSD', 'SUSHI-BNB', 'MANA-BUSD', 'DUSK-USDT', 'ETH-RUB', 'TOMO-USDT', 'KEY-USDT', 'SUPER-USDT', 'BTT-USDC', 'TKO-BIDR', 'MIR-BUSD', 'FTM-USDT', 'YFI-EUR', 'DNT-ETH', 'AMB-BTC', 'AION-BNB', 'RSR-BUSD', 'LOOM-ETH', 'POWR-BTC', 'ALGO-BUSD', 'ETH-BTC', 'TNT-BTC', 'STX-USDT', 'SYS-BTC', 'GBP-USDT', 'ZIL-BNB', 'LIT-USDT', 'ADA-AUD', 'OCEAN-BUSD', 'SHIB-BUSD', 'XRP-PAX', 'ARDR-USDT', 'WABI-BTC', 'LTC-RUB', 'PAXG-USDT', 'SLP-ETH', 'WTC-BNB', 'XLM-EUR', 'XRP-RUB', 'MANA-ETH', 'EVX-BTC', 'PIVX-ETH', 'POLY-BTC', 'ARN-ETH', 'BTS-ETH', 'NKN-USDT', 'HOT-USDT', 'VET-ETH', 'FIRO-USDT', 'MDA-ETH', 'MCO-ETH', 'BEL-BNB', 'UNFI-USDT', 'ZEC-ETH', 'ICP-USDT', 'NULS-ETH', 'BNB-BUSD', 'DOT-BNB', 'KNC-BUSD', 'PSG-BUSD', 'POND-USDT', 'IDEX-USDT', 'PPT-BTC', 'SUSHI-USDT', 'COMP-BTC', 'QSP-ETH', 'LINK-GBP', 'SNX-BUSD', 'TNB-BTC', 'CTK-USDT', 'TWT-USDT', 'BTC-BRL', 'BTS-BTC', 'DASH-BTC', 'MIR-USDT', 'CELO-USDT', 'STRAX-USDT', 'XVG-BTC', 'WRX-BTC', 'BTC-RUB', 'PROM-BUSD', 'ALICE-BTC', 'TRX-PAX', 'LTC-BUSD', 'BCH-BTC', 'LTC-USDC', 'DF-BUSD', 'NEO-BUSD', 'TFUEL-USDT', 'MTH-BTC', 'TRX-EUR', 'DIA-BUSD', 'DOGE-AUD', 'JST-BTC', 'COCOS-BNB', 'CHZ-BTC', 'FIO-BUSD', 'RCN-BTC', 'RVN-USDT', 'AXS-BNB', 'QLC-ETH', 'BCPT-BTC', 'BCPT-ETH', 'XZC-BTC', 'NEO-ETH', 'XTZ-BTC', 'QKC-ETH', 'ENJ-EUR', 'BAL-BUSD', 'FORTH-USDT', 'LUN-BTC', 'VIDT-BTC', 'LTO-USDT', '1INCH-BTC', 'IOTA-USDT', 'DOTUP-USDT', 'SOL-TRY', 'TRX-USDT', 'WAN-USDT', 'MBL-USDT', 'XEC-BUSD', 'TRX-BTC', 'MFT-USDT', 'LSK-USDT', 'FIO-BTC', 'GTO-BTC', 'BEL-BTC', 'CVC-BTC', 'HARD-USDT', 'BAT-BNB', 'DENT-USDT', 'VIB-BTC', 'ETC-BUSD', 'AUCTION-BUSD', 'CFX-USDT', 'FUN-ETH', 'ADA-TUSD', 'FLOW-USDT', 'CTK-BNB', 'NPXS-USDT', 'DLT-ETH', 'POE-BTC', 'YFII-BUSD', 'BAT-USDT', 'CRV-BTC', 'CMT-ETH', 'COTI-BTC', 'PUNDIX-USDT', 'SXP-BUSD', 'LTC-TUSD', 'WABI-ETH', 'TRX-TUSD', 'KLAY-USDT', 'LRC-BTC', 'BNBDOWN-USDT', 'TRX-USDC', 'AERGO-BUSD', 'LIT-BUSD', 'VTHO-USDT', 'ZIL-BUSD', 'BNB-TRY', 'QLC-BTC', 'BETA-USDT', 'DATA-BUSD', 'WAVES-USDT', 'MDT-BTC', 'LSK-BTC', 'AGI-BTC', 'ETH-DAI', 'MATIC-USDT', 'SNT-BTC', 'ETH-TRY', 'DOT-USDT', 'BTT-TRX', 'NBS-USDT', 'STORM-ETH', 'UNI-BUSD', 'INJ-BUSD', 'ALPHA-BTC', 'KMD-BTC', 'VET-TRY', 'AE-BTC', 'JST-USDT', 'NEAR-BUSD', 'AVA-USDT', 'NAS-ETH', 'ONG-BTC', 'PHB-BTC', 'THETA-BUSD', 'TUSD-USDT', 'VIDT-BUSD', 'BTT-BNB', 'BUSD-BIDR', 'WTC-ETH', 'ZEC-USDT', 'WPR-BTC', 'MDT-USDT', 'MINA-USDT', 'VIBE-BTC', 'OGN-BNB', 'SLP-USDT', 'CHR-BTC', 'DASH-USDT', 'COS-BTC', 'BUSD-BRL', 'NAS-BTC', 'XLM-BTC', 'XTZ-BUSD', 'COMP-USDT', 'TROY-BNB', 'REQ-ETH', 'WRX-BNB', 'XVS-USDT', 'GTO-USDT', 'HARD-BTC', 'BTC-USDC', 'MCO-BTC', 'ARK-BTC', 'KP3R-BUSD', 'MITH-BTC', 'BAND-USDT', 'WAN-ETH', 'CELO-BTC', 'CTSI-BNB', 'CHZ-EUR', 'ETH-USDC', 'BTCST-BTC', 'NULS-USDT', 'STRAX-BUSD', 'AION-BTC', 'LINK-TUSD', 'COCOS-USDT', 'BNBUP-USDT', 'UNI-BNB', 'XRP-BRL', 'WBTC-ETH', 'BCH-USDT', 'BNB-IDRT', 'DOT-EUR', 'MKR-BUSD', 'RVN-BUSD', 'ICX-ETH', 'LTC-BRL', 'ZIL-USDT', 'IQ-BUSD', 'WRX-USDT', 'AAVE-ETH', 'CTSI-BTC', 'SLP-BUSD', 'ALGO-BNB', 'BNB-AUD', 'SAND-USDT', 'DASH-BNB', 'ADA-ETH', 'OMG-USDT', 'ADA-BRL', 'INJ-BNB', 'KNC-BTC', 'NEO-USDT', 'OAX-BTC', 'BTS-USDT', 'IRIS-USDT', 'DODO-BUSD', 'DCR-BTC', 'LAZIO-TRY', 'ONT-BTC', 'STPT-BTC', 'DOTDOWN-USDT', 'PPT-ETH', 'ALPHA-USDT', 'DATA-ETH', 'SOL-EUR', 'UNI-USDT', 'ANT-BUSD', 'LTCDOWN-USDT', 'ROSE-BTC', 'XMR-BNB', 'BAT-BTC', 'ICX-BUSD', 'ETH-AUD', 'ATOM-USDT', 'BNB-DAI', 'DOGE-BRL', 'PERL-USDT', 'UFT-BUSD', 'WIN-USDT', 'ARPA-BTC', 'DOGE-BTC', 'SOL-BTC', 'XRP-ETH', 'USDT-BRL', 'XVS-BUSD', 'AVAX-TRY', 'MBOX-USDT', 'KAVA-USDT', 'AVAX-USDT', 'BNB-TUSD', 'WING-USDT', 'ADADOWN-USDT', 'RDN-BTC', 'AION-USDT', 'EOS-BNB', 'SHIB-TRY', 'USDC-USDT', 'OGN-BTC', 'GVT-BTC', 'ENJ-BNB', 'BAKE-BUSD', 'SRM-USDT', 'VET-BUSD', 'HC-BTC', 'XRP-BNB', 'BAR-USDT', 'SHIB-EUR', 'ONE-USDT', 'STEEM-ETH', 'DOCK-USDT', 'DATA-USDT', 'STORJ-USDT', 'YFI-USDT', 'BTT-TUSD', 'TRB-BUSD', 'WAVES-ETH', 'INS-BTC', 'AUTO-USDT', 'BNB-EUR', 'MFT-BNB', 'NEO-BNB', 'WABI-BNB', 'MFT-ETH', 'FTT-BTC', 'DOT-BIDR', 'POLS-USDT', 'SNX-BTC', 'ALGO-BTC', 'LPT-USDT', 'FXS-BUSD', 'WIN-EUR', 'REEF-USDT', 'EGLD-BNB', 'BCC-BTC', 'RDN-ETH', 'ZIL-BTC', 'NANO-ETH', 'ENJ-ETH', 'CKB-BUSD', 'GNT-ETH', 'XEM-BTC', 'HNT-BTC', 'CHZ-USDT', 'MKR-BTC', 'XLM-BUSD', 'FET-USDT', 'BNB-BRL', 'RVN-BNB', 'QTUM-BTC', 'ZRX-BTC', 'AR-BTC', 'INJ-BTC', 'PAXG-BTC', 'USDT-NGN', 'LINA-USDT', 'TRX-BUSD', 'SXP-TRY', 'TRB-BTC', 'DGB-USDT', 'VIA-BTC', 'ENJ-BTC', 'EDO-BTC', 'IOTX-ETH', 'AUD-BUSD', 'PNT-BTC', 'TNB-ETH', 'VITE-BTC', 'REP-ETH', 'SRM-BNB', 'TKO-USDT', 'NAV-BTC', 'ALICE-BUSD', 'IOST-BUSD', 'FTM-BNB', 'REN-BTC', 'RUNE-USDT', 'XVS-BNB', 'IOTX-BTC', 'FOR-BUSD', 'RVN-TRY', 'USDC-BUSD', 'ETC-BNB', 'ETH-BRL', 'STORM-BTC', 'MTL-USDT', 'XRPDOWN-USDT', 'BLZ-ETH', 'CAKE-BUSD', 'ELF-BTC', '1INCH-BUSD', 'BTCST-BUSD', 'DOGE-EUR', 'ETH-EUR', 'SYS-BUSD', 'MANA-USDT', 'MATIC-EUR', 'IOTA-ETH', 'SC-BTC', 'RUNE-BTC', 'CMT-BTC', 'TRXDOWN-USDT', 'PAX-USDT', 'BTC-BIDR', 'KNC-USDT', 'AAVE-BTC', 'ERD-USDT', 'MBL-BNB', 'REQ-BTC', 'PERL-BTC', 'AE-ETH', 'ICP-BTC', 'BAKE-USDT', 'ELF-ETH', 'TLM-USDT', 'ENG-BTC', 'RLC-USDT', 'LINK-EUR', 'AAVE-USDT', 'TCT-BTC', 'WING-BUSD', 'DNT-BTC', 'LEND-BTC', 'XRP-BTC', 'SRM-BTC', 'DREP-USDT', 'FIO-USDT', 'AMB-ETH', 'NEAR-USDT', 'DOGE-USDT', 'KNC-ETH', 'EGLD-EUR', 'NANO-USDT', 'BZRX-USDT', 'BAND-BTC', 'NKN-BTC', 'CHZ-BUSD', 'BCH-USDC', 'WIN-USDC', 'CREAM-BUSD', 'BCD-BTC', 'ARN-BTC', 'XRP-TUSD', 'KMD-USDT', 'SAND-BUSD', 'LTO-BTC', 'OCEAN-BTC', 'ETC-USDT', 'XRP-AUD', 'DEGO-BUSD', 'ATOM-BTC', 'WIN-BNB', 'ETH-TUSD', 'VET-BNB', 'WIN-BUSD', 'CAKE-BTC', 'CELR-BUSD', 'XLM-USDT', 'GRT-EUR', 'DYDX-USDT', 'KMD-ETH', 'LINK-BTC', 'AKRO-USDT', 'XRP-EUR', 'AAVE-BUSD', 'COTI-BNB', 'FIL-USDT', 'NEBL-BTC', 'XRP-USDT', 'AST-ETH', 'STX-BTC', 'WRX-BUSD', 'LINK-BRL', 'SUSHIDOWN-USDT', 'CELR-BNB', 'LTC-ETH', 'WAVES-BUSD', 'XTZ-USDT', 'GALA-USDT', 'SOL-BUSD', 'NULS-BTC', 'COMP-BUSD', 'XRP-USDC', 'BCH-BNB', 'INJ-USDT', 'AUDIO-USDT', 'GXS-ETH', 'BLZ-BNB', 'AVAX-EUR', 'ATOM-USDC', 'SUPER-BUSD', 'XVS-BTC', 'BCC-USDT', 'ANT-BTC', 'DNT-USDT', 'HOT-ETH', 'BADGER-USDT', 'BTC-TRY', 'MATIC-BTC', 'PHA-BUSD', 'BUSD-TRY', 'VET-BTC', 'KSM-USDT', 'TFUEL-BTC', 'ADA-BTC', 'DENT-ETH', 'XEC-USDT', 'XTZ-BNB', 'NANO-BTC', 'ENJ-BUSD', 'ADX-BTC', 'TOMO-BTC', 'EOS-BTC', 'BTC-DAI', 'DOGE-TRY', 'LINK-BUSD', 'ORN-USDT', 'SUSHIUP-USDT', 'DEGO-USDT', 'RVN-BTC', 'SUN-USDT', 'XRP-BUSD', 'ENG-ETH', 'AVA-BUSD', 'OST-BTC', 'BTG-ETH', 'ANT-USDT', 'EOS-BUSD', 'IOTA-BTC', 'ONG-USDT', 'GO-BTC', 'FUEL-BTC', 'OXT-BTC', 'UNI-BTC', 'TNT-ETH', 'HOT-BTC', 'UMA-BTC', 'MKR-USDT', 'XEM-USDT', 'NEAR-BNB', 'MATIC-BUSD', 'ZEC-BTC', 'SKY-BTC', 'BNB-BTC', 'UTK-BTC', 'HNT-USDT', 'GTO-ETH', 'USDT-TRY', 'RUNE-BNB', 'BNB-GBP', 'STRAX-BTC', 'DGB-BUSD', 'ANKR-BNB', 'NXS-BTC', 'KSM-BTC', 'TVK-BUSD', 'ONT-USDT', 'ATOM-BNB', 'CND-BTC', 'NCASH-BTC', 'AVAX-BUSD', 'NMR-BTC', 'WTC-USDT', 'POA-BTC', 'TLM-BUSD', 'STMX-ETH', 'QSP-BTC', 'XMR-BUSD', 'COS-USDT', 'ENJ-USDT', 'LRC-USDT', 'LSK-ETH', 'WTC-BTC', 'COTI-USDT', 'REN-USDT', 'LUNA-EUR', 'DOT-BTC', 'FLM-BTC', 'DGD-ETH', 'BLZ-USDT', 'BRD-ETH', 'BTCST-USDT', 'PNT-USDT', 'DOGE-BUSD', 'ONE-BTC', 'NCASH-ETH', 'AXS-BUSD', 'BNB-USDC', 'AVA-BTC', 'EVX-ETH', 'CAKE-BNB', 'BCH-BUSD', 'BCHABC-USDT', 'BTG-BTC', 'CELR-BTC', 'XVG-USDT', 'EUR-BUSD', 'ICX-USDT', 'BNB-RUB', 'HBAR-USDT', 'OCEAN-USDT', 'HOT-BUSD', '1INCH-USDT', 'SCRT-BTC', 'BUSD-RUB', 'TRX-BNB', 'POWR-ETH', 'SNGLS-BTC', 'THETA-ETH', 'LUNA-BNB', 'XRP-TRY', 'VIB-ETH', 'RAMP-USDT', 'POE-ETH', 'BEAM-USDT', 'BTCUP-USDT', 'TWT-BUSD', 'NMR-BUSD', 'LUNA-BTC', 'SKL-USDT', 'SOL-BIDR', 'KEY-ETH', 'SNT-ETH', 'WAVES-BTC', 'NANO-BUSD', 'AXS-BTC', 'MTL-BTC', 'TCT-USDT', 'GBP-BUSD', 'BAT-ETH', 'LINA-BUSD', 'FTT-BUSD', 'FIL-BTC', 'ALGO-USDT', 'GLM-BTC', 'ETC-ETH', 'ATM-USDT', 'PROS-ETH', 'THETA-USDT', 'SAND-BTC', 'DIA-USDT', 'USDT-DAI', 'XRP-GBP', 'NEAR-BTC', 'BNB-USDT', 'BNB-PAX', 'ADA-USDT', 'MDX-USDT', 'XTZDOWN-USDT', 'EUR-USDT', 'FRONT-BUSD', 'ONE-BUSD', 'PIVX-BTC', 'CRV-USDT', 'ANKR-BTC', 'DOT-BUSD', 'DCR-USDT', 'FTM-BUSD', 'GXS-USDT', 'AUDIO-BUSD', 'LUNA-USDT', 'OAX-ETH', 'WNXM-USDT', 'GRT-ETH', 'TRX-XRP', 'OGN-USDT', 'APPC-BTC', 'EOS-ETH', 'XMR-USDT', 'ZEN-BTC', 'ADA-GBP', 'ATOM-BUSD', 'IOST-BNB', 'ORN-BTC', 'EOS-TRY', 'LINK-ETH', 'ARPA-USDT', 'ETH-GBP', 'LRC-ETH', 'WIN-TRX', 'XTZUP-USDT', 'TRX-ETH', 'EPS-USDT', 'LINK-USDC', 'QTUM-BUSD', 'BTT-USDT', 'VITE-USDT', 'KSM-BUSD', 'CTSI-USDT', 'SUSHI-BTC', 'BRD-BTC', 'BTG-USDT', 'ZIL-ETH', 'CTSI-BUSD', 'COS-BNB', 'DAR-USDT', 'STEEM-BTC', 'GRT-USDT', 'ADA-EUR', 'DYDX-BUSD', 'BTC-EUR', 'HOT-TRY', 'ADA-BNB', 'GVT-ETH', 'ACM-USDT', 'ADA-TRY', 'FET-BNB', 'XMR-BTC', 'OMG-BUSD', 'NPXS-ETH', 'SRM-BUSD', 'RLC-BTC', 'AXS-USDT', 'CKB-USDT', 'IDEX-BUSD', 'ARDR-BTC', 'IOTX-BUSD', 'STPT-USDT', 'LTC-EUR', 'BTC-AUD', 'MATIC-BNB', 'SKL-BTC', 'SC-USDT', 'JST-BUSD', 'CHZ-TRY', 'CTXC-BTC', 'HIVE-BTC', 'ZRX-BUSD', 'DUSK-BTC', 'BNB-BIDR', 'EGLD-USDT', 'ROSE-USDT', 'USDT-RUB', 'STORJ-BTC', 'ALICE-USDT', 'WAN-BNB', 'GRS-BTC', 'LINKUP-USDT', 'THETA-BNB', 'SXP-USDT', 'XVG-ETH', 'ADA-BUSD', 'BIFI-BUSD', 'WAVES-BNB', 'ARPA-BNB', 'ATA-USDT', 'CTXC-USDT', 'FTM-BTC', 'QTUM-USDT', 'BNT-USDT', 'HARD-BUSD', 'AVAX-BTC', 'BTC-USDT', 'AR-USDT', 'REEF-BUSD', 'BAND-BUSD', 'BCD-ETH', 'CAKE-USDT', 'BTC-UAH', 'USDT-IDRT', 'ETH-USDT', 'RLC-ETH', 'ETH-PAX', 'FUN-BTC', 'DGD-BTC', 'FUN-USDT', 'QTUM-ETH', 'ZEC-BUSD', 'BAKE-BNB', 'MDA-BTC', 'BTC-BUSD', 'DATA-BTC', 'DIA-BTC', 'BTC-GBP', 'CHR-BNB', 'YFII-USDT', 'ZEC-BNB', 'BLZ-BTC', 'YFII-BTC', 'AR-BUSD', 'BAT-BUSD', 'XMR-ETH', 'STX-BNB', 'QNT-USDT', 'ONT-BNB', 'BTT-BUSD', 'LTC-BNB', 'FTT-USDT', 'SXP-BNB', 'TROY-USDT', 'PSG-USDT', 'DGB-BTC', 'CHZ-BRL', 'ICP-BUSD', 'ETHDOWN-USDT', 'BAL-USDT', 'BETH-ETH', 'BQX-ETH', 'TRU-USDT', 'DOCK-BTC', 'LEND-ETH', 'LIT-BTC', 'NEO-BTC', 'BNB-ETH', 'AION-ETH', 'CELR-USDT', 'ETH-BIDR', 'DEXE-BUSD', 'CDT-BTC', 'UNFI-BUSD', 'YFI-BTC', 'OXT-USDT', 'WIN-BRL', 'GHST-BUSD', 'GRT-BTC', 'HIVE-USDT', 'LTC-USDT', 'TOMO-BUSD', 'CDT-ETH', 'ADAUP-USDT', 'SOL-BNB', 'GTC-USDT', 'STRAT-BTC', 'STRAT-ETH', 'XEM-ETH', 'ZEN-BNB', 'LINK-USDT', 'NEBL-ETH', 'FET-BTC', 'GNT-BTC', 'SPARTA-BNB', 'ETC-BTC', 'JUV-USDT', 'ARK-ETH', 'FTT-BNB', 'STORJ-ETH', 'SHIB-USDT', 'ZEN-USDT', 'USDT-BIDR', 'LTCUP-USDT', 'BEL-BUSD', 'IOST-USDT', 'ETH-BUSD', 'VET-USDT', 'BURGER-USDT', 'IRIS-BTC', 'CRV-BUSD', 'WAN-BTC', 'ZEC-USDC', 'THETA-BTC', 'OM-USDT', 'BNT-BTC', 'CVC-ETH', 'OMG-ETH', 'MITH-USDT', 'SXPUP-USDT', 'DOGE-GBP', 'ONT-BUSD', 'SOL-USDT', 'BQX-BTC', 'BTC-TUSD', 'CTK-BTC', 'BNT-BUSD', 'GAS-BTC', 'XVG-BUSD', 'IDEX-BTC', 'ZEN-ETH', 'CHR-USDT', 'POND-BUSD', 'ICX-BNB', 'HBAR-BTC', 'EOS-USDT', 'ONE-BNB', 'UMA-USDT', 'ANKR-USDT', 'USDT-UAH', 'ZRX-USDT', 'GXS-BTC', 'RUNE-BUSD', 'REP-USDT', 'SKL-BUSD', 'ADA-USDC', 'WING-BTC', 'XLM-BNB', 'ASR-USDT', 'ZIL-BIDR', 'DOT-TRY', 'MANA-BTC', 'BEL-USDT', 'DLT-BTC', 'SHIB-BRL', 'IOST-BTC', 'GRT-BUSD', 'FIS-USDT', 'WBTC-BTC', 'BUSD-USDT', 'IOTA-BUSD', 'DREP-BTC', 'AAVE-BNB', 'DODO-USDT', 'GALA-BUSD', 'LUNA-BUSD', 'XZC-ETH', 'LINK-TRY', 'AVAX-BNB', 'BTC-NGN', 'AUD-USDT', 'CHZ-BNB', 'HOT-BNB', 'ICX-BTC', 'BEAM-BTC', 'SC-BNB', 'RIF-USDT', 'REP-BTC', 'SXP-EUR', 'ARPA-TRY', 'CVC-USDT', 'KAVA-BNB', 'CTK-BUSD', 'STMX-USDT', 'TKO-BUSD', 'PERL-BNB', 'XLM-TRY', 'ERN-USDT', 'ADX-ETH', 'HBAR-BUSD', 'SFP-USDT', 'SNM-BTC', 'IOST-ETH', 'DASH-BUSD', 'CVP-BUSD', 'ONT-ETH', 'BUSD-DAI', 'OMG-BTC', 'SC-ETH', 'SUSHI-BUSD', 'TRB-USDT', 'BAL-BTC', 'UNFI-BTC', 'VET-EUR', 'TRXUP-USDT', 'FIL-BNB', 'SFP-BUSD', 'ETHUP-USDT', 'MATIC-TRY', 'FIL-BUSD', 'UTK-USDT', 'BTC-PAX', 'SNX-USDT', 'NMR-USDT', 'FXS-BTC', 'LINKDOWN-USDT', 'ALPHA-BUSD', 'OG-USDT', 'AUDIO-BTC', 'SXP-BTC', 'FLM-USDT', 'RSR-USDT', 'PERP-USDT', 'EGLD-BTC', 'YOYO-BTC'}\n"
     ]
    }
   ],
   "source": [
    "symbols = set()\n",
    "for f in all_parquet_files:\n",
    "    match = re.search(r'([^/\\\\]+)\\.parquet$', f)\n",
    "    if match:\n",
    "        symbols.add(match.group(1))\n",
    "print(symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample percentage control for faster experimentation and debugging on smaller data subsets before running full training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using full dataset\n"
     ]
    }
   ],
   "source": [
    "USE_SAMPLE = False  \n",
    "SAMPLE_PERCENTAGE = 0.2\n",
    "if USE_SAMPLE:\n",
    "    train_files = train_files[:int(len(train_files) * SAMPLE_PERCENTAGE)]\n",
    "    test_files = test_files[:int(len(test_files) * SAMPLE_PERCENTAGE)]\n",
    "    print(f\"Using sample - Train: {len(train_files)} files, Test: {len(test_files)} files\")\n",
    "else:\n",
    "    print(f\"Using full dataset\")\n",
    "train_files_full = [os.path.join(DATA_FOLDER, f) for f in train_files]\n",
    "test_files_full = [os.path.join(DATA_FOLDER, f) for f in test_files]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the data and adding a symbol column to datasets. First instance of gc.collect() used repeatedly throughout the code to force freeup memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory: 79.7% used (13.1GB/16.5GB)\n",
      "Swap: 32.8% used (7.4GB/22.5GB) \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "127"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = spark.read.parquet(*train_files_full)\n",
    "df_test = spark.read.parquet(*test_files_full)\n",
    "\n",
    "df_train = df_train.withColumn(\n",
    "    \"symbol\",\n",
    "    regexp_extract(input_file_name(), r'([^/\\\\]+)\\.parquet$', 1)\n",
    ")\n",
    "\n",
    "df_test = df_test.withColumn(\n",
    "    \"symbol\",\n",
    "    regexp_extract(input_file_name(), r'([^/\\\\]+)\\.parquet$', 1)\n",
    ")\n",
    "\n",
    "check_memory()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregates raw 1-minute candlestick data into larger timeframes defined by CANDLE_MINUTES variable. Creates time buckets by flooring timestamps to the nearest interval, then groups by symbol and bucket. Applies OHLCV aggregation rules: first open price of the period, maximum high reached, minimum low touched, last close price, and sum of volume and number of trades. Checkpoints after aggregation to materialize the result to disk, breaking the query lineage to prevent stack overflow errors on complex transformation chains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory: 82.5% used (13.6GB/16.5GB)\n",
      "Swap: 32.4% used (7.3GB/22.5GB) \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def aggregate_candles(df):\n",
    "    df = df.withColumn(\n",
    "        \"time_bucket\",\n",
    "        floor(unix_timestamp(col(\"open_time\")) * 1000 / lit(CANDLE_MINUTES*60*1000)) * lit(CANDLE_MINUTES*60*1000)\n",
    "    )\n",
    "    \n",
    "    df = df.groupBy(\"symbol\", \"time_bucket\").agg(\n",
    "        first(\"open\").alias(\"open\"),\n",
    "        _max(\"high\").alias(\"high\"),\n",
    "        _min(\"low\").alias(\"low\"),\n",
    "        last(\"close\").alias(\"close\"),\n",
    "        _sum(\"volume\").alias(\"volume\"),\n",
    "        _sum(\"number_of_trades\").alias(\"number_of_trades\"),\n",
    "        #_sum(\"taker_buy_base_asset_volume\").alias(\"taker_buy_base_asset_volume\"), not used\n",
    "        _sum(\"taker_buy_quote_asset_volume\").alias(\"taker_buy_quote_asset_volume\")\n",
    "    ).withColumnRenamed(\"time_bucket\", \"open_time\").orderBy(\"symbol\", \"open_time\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_train = aggregate_candles(df_train)\n",
    "df_test = aggregate_candles(df_test)\n",
    "\n",
    "df_train = df_train.checkpoint()\n",
    "df_test = df_test.checkpoint()\n",
    "check_memory()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generates technical indicators and derived features from raw OHLCV data. The feature engineering pipeline creates lag features (previous period values), moving averages (SMA 5, 10, 20), candlestick patterns (body, wicks, range), momentum indicators, Bollinger Band position, and Average True Range (ATR). Features used in the final model training are chosen through trial and error, through multiple training cycles and smaller data samples, by removing underperforming features based on the resulting AUC until further removal worsened the model performance. The final feature set includes: moving average ratios (price_to_sma5/10/20), price momentum, candlestick patterns (body, range, upper_wick, lower_wick),some lag features for high/close/open prices, trading volume metrics (number_of_trades, taker_buy_quote_asset_volume), and volatility indicators (bb_position, atr_10). Features are assembled into Spark ML vectors and checkpointed to prevent recomputation during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start feature generation for TRAINING\n",
      "Memory: 84.2% used (13.9GB/16.5GB)\n",
      "Swap: 32.5% used (7.3GB/22.5GB) \n",
      "Memory: 84.1% used (13.9GB/16.5GB)\n",
      "Swap: 32.5% used (7.3GB/22.5GB) \n",
      "Start feature generation for TEST\n",
      "Memory: 85.5% used (14.1GB/16.5GB)\n",
      "Swap: 32.5% used (7.3GB/22.5GB) \n",
      "Memory: 85.3% used (14.1GB/16.5GB)\n",
      "Swap: 32.5% used (7.3GB/22.5GB) \n",
      "Memory: 87.4% used (14.4GB/16.5GB)\n",
      "Swap: 32.6% used (7.3GB/22.5GB) \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "318"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#feature engineering\n",
    "def generate_features(df, dataset_name=\"\"):\n",
    "    print(f\"Start feature generation for {dataset_name}\")\n",
    "    window_symbol = Window.partitionBy(\"symbol\").orderBy(\"open_time\")\n",
    "    def drop_unused_columns(df, keep_list):\n",
    "        #Dropping all columns except those in keep_list (and system columns)\n",
    "        system_cols = [\"symbol\", \"open_time\", \"label\"]\n",
    "        keep_cols = set(keep_list + system_cols)\n",
    "        drop_cols = [c for c in df.columns if c not in keep_cols]\n",
    "        \n",
    "        if drop_cols:\n",
    "            df = df.drop(*drop_cols)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    window_spec = Window.partitionBy(\"symbol\").orderBy(\"open_time\")\n",
    "    lag_config = {\n",
    "        \"high\": [1, 2, 3, 4, 5, 6],  \n",
    "        \"close\": LAG_INTERVALS, \n",
    "        \"open\": [3, 5, 6], \n",
    "        \"number_of_trades\": [1, 2, 3],\n",
    "        \"taker_buy_quote_asset_volume\": [1, 2, 3]  \n",
    "    }\n",
    "\n",
    "    for column, lags in lag_config.items():\n",
    "        for lag_period in lags:\n",
    "            df = df.withColumn(\n",
    "                f\"{column}_lag{lag_period}\", \n",
    "                lag(col(column), lag_period).over(window_spec)\n",
    "            )\n",
    "    df = df.checkpoint()\n",
    "    check_memory()\n",
    "    gc.collect()\n",
    "    \n",
    "    #label\n",
    "    df = df.withColumn(\"close_next\", lead(col(\"close\"), 1).over(window_spec))\n",
    "    df = df.withColumn(\"label\",\n",
    "        when(col(\"close_next\") > col(\"close\"), 1)\n",
    "        .when(col(\"close_next\") < col(\"close\"), 0)\n",
    "        .otherwise(0)\n",
    "    )\n",
    "    df = df.drop(\"close_next\")\n",
    "    \n",
    "    #derived features\n",
    "    df = df.withColumn(\"body\", col(\"close\") - col(\"open\"))\n",
    "    df = df.withColumn(\"range\", col(\"high\") - col(\"low\"))\n",
    "    df = df.withColumn(\"upper_wick\", col(\"high\") - when(col(\"close\") > col(\"open\"), col(\"close\")).otherwise(col(\"open\")))\n",
    "    df = df.withColumn(\"lower_wick\", when(col(\"close\") < col(\"open\"), col(\"close\")).otherwise(col(\"open\")) - col(\"low\"))\n",
    "    \n",
    "    #Simple moving averages\n",
    "    #SMA-5\n",
    "    df = df.withColumn(\"sma_5\", \n",
    "        (col(\"close_lag1\") + col(\"close_lag2\") + col(\"close_lag3\") + col(\"close_lag4\") + col(\"close_lag5\")) / 5)\n",
    "    df = df.withColumn(\"price_to_sma5\", \n",
    "        when(col(\"sma_5\") != 0, (col(\"close\") - col(\"sma_5\")) / col(\"sma_5\")).otherwise(0))\n",
    "    df = df.drop(\"sma_5\")\n",
    "    #SMA-10\n",
    "    close_lags_10 = [col(f\"close_lag{i}\") for i in range(1, 11)]\n",
    "    df = df.withColumn(\"sma_10\", reduce(lambda a, b: a + b, close_lags_10) / lit(10))\n",
    "    df = df.withColumn(\"price_to_sma10\", \n",
    "        when(col(\"sma_10\") != 0, (col(\"close\") - col(\"sma_10\")) / col(\"sma_10\")).otherwise(0))\n",
    "    df = df.drop(\"sma_10\")\n",
    "    #SMA-20\n",
    "    close_lags_20 = [col(f\"close_lag{i}\") for i in range(1, 21)]\n",
    "    df = df.withColumn(\"sma_20\", reduce(lambda a, b: a + b, close_lags_20) / lit(20))\n",
    "    df = df.withColumn(\"price_to_sma20\", \n",
    "        when(col(\"sma_20\") != 0, (col(\"close\") - col(\"sma_20\")) / col(\"sma_20\")).otherwise(0))\n",
    "    #momentum\n",
    "    df = df.withColumn(\"price_momentum\", \n",
    "        when(col(\"close_lag5\") != 0, (col(\"close\") - col(\"close_lag5\")) / col(\"close_lag5\")).otherwise(0))\n",
    "    \n",
    "    #Bollinger Bands position\n",
    "    df = df.withColumn(\"volatility\",\n",
    "        sqrt((\n",
    "            pow((col(\"close_lag1\") - col(\"close_lag2\")) / col(\"close_lag2\"), 2) +\n",
    "            pow((col(\"close_lag2\") - col(\"close_lag3\")) / col(\"close_lag3\"), 2) +\n",
    "            pow((col(\"close_lag3\") - col(\"close_lag4\")) / col(\"close_lag4\"), 2)\n",
    "        ) / 3))\n",
    "    df = df.withColumn(\"bb_position\",\n",
    "        when(col(\"volatility\") != 0,\n",
    "            (col(\"close\") - col(\"sma_20\")) / (2 * col(\"volatility\")))\n",
    "        .otherwise(0))\n",
    "    df = df.drop(\"volatility\", \"sma_20\")\n",
    "\n",
    "    #average true range\n",
    "    df = df.withColumn(\"true_range\",\n",
    "        greatest(\n",
    "            col(\"high\") - col(\"low\"),\n",
    "            sabs(col(\"high\") - lag(col(\"close\"), 1).over(window_symbol)),\n",
    "            sabs(col(\"low\") - lag(col(\"close\"), 1).over(window_symbol))\n",
    "        )\n",
    "    )\n",
    "    window_tr = Window.partitionBy(\"symbol\").orderBy(\"open_time\").rowsBetween(-10, -1)\n",
    "    df = df.withColumn(\"atr_10\", avg(col(\"true_range\")).over(window_tr))\n",
    "    df = df.drop(\"true_range\")\n",
    "\n",
    "    df = df.dropna()\n",
    "    check_memory()\n",
    "    gc.collect()\n",
    "\n",
    "    #assembling features\n",
    "    SELECTED_FEATURES = [\n",
    "        \"price_to_sma5\", \"price_to_sma10\", \"price_momentum\", \"price_to_sma20\",\n",
    "        \n",
    "        \"body\", \"range\", \"upper_wick\", \"lower_wick\",\n",
    "        \n",
    "        \"high_lag5\", \"high_lag6\", \"high_lag1\", \"high_lag2\", \"high_lag3\", \"high_lag4\",\n",
    "        \"close_lag3\", \"close_lag4\",\n",
    "        \"open_lag6\", \"open_lag3\", \"open_lag5\",\n",
    "        \"number_of_trades_lag1\", \"number_of_trades_lag2\", \n",
    "        \"number_of_trades_lag3\",\n",
    "        \"taker_buy_quote_asset_volume_lag1\",\n",
    "        \"taker_buy_quote_asset_volume_lag2\",\n",
    "        \"taker_buy_quote_asset_volume_lag3\",\n",
    "        \n",
    "        \"bb_position\", \"atr_10\"\n",
    "    ]\n",
    "    \n",
    "    df = drop_unused_columns(df, SELECTED_FEATURES)\n",
    "\n",
    "    assembler = VectorAssembler(inputCols=SELECTED_FEATURES, outputCol=\"features\", handleInvalid=\"skip\")\n",
    "    df = assembler.transform(df)\n",
    "\n",
    "    result_df = df.select(\"symbol\", \"features\", \"label\", \"open_time\")\n",
    "    \n",
    "    return result_df, SELECTED_FEATURES\n",
    "\n",
    "# Generate features for train and test\n",
    "df_train_features, SELECTED_FEATURES = generate_features(df_train, \"TRAINING\")\n",
    "df_test_features, _ = generate_features(df_test, \"TEST\")\n",
    "\n",
    "# Checkpoint before training\n",
    "df_train_features = df_train_features.checkpoint()\n",
    "df_test_features = df_test_features.checkpoint()\n",
    "check_memory()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzes the distribution of target labels (UP vs DOWN price movements) in the training set and calculates class weights to handle imbalance.Computes the proportion of each class and applies inverse frequency weighting. A downscale factor (set to 1.0 for full dataset) allows fine-tuning the weight  to prevent over-correction. These weights are added as a column to the training data, to make minority calss receive higher importance during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOWN (0): 22,021,062 (55.54%)\n",
      "UP (1): 17,628,667 (44.46%)\n",
      "\n",
      "Class weights:\n",
      "DOWN (0): 0.9003\n",
      "UP (1): 1.1246\n"
     ]
    }
   ],
   "source": [
    "label_counts = df_train_features.groupBy(\"label\").count().collect()\n",
    "label_dict = {row['label']: row['count'] for row in label_counts}\n",
    "\n",
    "count_0 = label_dict.get(0, 0)\n",
    "count_1 = label_dict.get(1, 0)\n",
    "total = count_0 + count_1\n",
    "\n",
    "print(f\"DOWN (0): {count_0:,} ({count_0/total*100:.2f}%)\")\n",
    "print(f\"UP (1): {count_1:,} ({count_1/total*100:.2f}%)\")\n",
    "\n",
    "if count_0 > 0 and count_1 > 0:\n",
    "    raw_weight_0 = total / (2.0 * count_0)\n",
    "    raw_weight_1 = total / (2.0 * count_1)\n",
    "    downscale_factor = 1 #left at neutral 1 for full dataset\n",
    "    weight_0 = 1.0 + (raw_weight_0 - 1.0) * downscale_factor\n",
    "    weight_1 = 1.0 + (raw_weight_1 - 1.0) * downscale_factor\n",
    "    \n",
    "    print(f\"\\nClass weights:\")\n",
    "    print(f\"DOWN (0): {weight_0:.4f}\")\n",
    "    print(f\"UP (1): {weight_1:.4f}\")\n",
    "    \n",
    "    df_train_features = df_train_features.withColumn(\n",
    "        \"weight\",\n",
    "        when(col(\"label\") == 0, lit(weight_0))\n",
    "        .when(col(\"label\") == 1, lit(weight_1))\n",
    "        .otherwise(lit(1.0))\n",
    "    )\n",
    "    use_weights = True\n",
    "else:\n",
    "    use_weights = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of time-series aware walk forward cross-validation to prevent data leakage and maintain temporal ordering. Training data is split into 3 folds using ntile partitioning.  Fold 0 (earliest data) is always included in training, while subsequent folds serve as validation sets in sequence. This ensures models are always trained on past data and validated on future data. For each fold three models are trained independently:\n",
    "1. Random forest: 50 trees, max depth 6, 70% subsampling, with class weights (args were chosen with a separate hyperparameter tuning script , training an RF on the same data sample with different combinations to find better conditions)\n",
    "2. Gradient boosted trees: 50 iterations, max depth 5, 0.1 learning rate. Since GBT don't natively support class weights, undersampling was used to account for class imbalance.\n",
    "3. Logistic regression: L2 regularization (regParam=0.01), 100 max iterations, with class weights.\n",
    "\n",
    "Each model generates probability predictions for its validation fold. The predictions are joined by row_id to create a combined dataset containing rf_prob, gbt_prob, and lr_prob columns. Out-of-fold predictions from all validation folds are accumulated for meta-learner training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold Temporal Validation ===\n",
      "+----+-------------+-------------+--------+\n",
      "|fold|   start_date|     end_date|   count|\n",
      "+----+-------------+-------------+--------+\n",
      "|   0|1500040800000|1596871800000|13216577|\n",
      "|   1|1596871800000|1633696200000|13216576|\n",
      "|   2|1633696200000|1668722400000|13216576|\n",
      "+----+-------------+-------------+--------+\n",
      "\n",
      "\n",
      "Skipping Fold 1 (earliest data - need for training)\n",
      "Fold 2/3\n",
      "RF trained in 12.9 mins\n",
      "GBT trained in 9.2 mins\n",
      "LR trained in3.0 mins\n",
      "Fold 1 complete\n",
      "Memory: 89.8% used (14.8GB/16.5GB)\n",
      "Swap: 33.6% used (7.6GB/22.5GB) \n",
      "Fold 3/3\n",
      "RF trained in 26.5 mins\n",
      "GBT trained in 28.4 mins\n",
      "LR trained in8.4 mins\n",
      "Fold 2 complete\n",
      "Memory: 68.6% used (11.3GB/16.5GB)\n",
      "Swap: 33.4% used (7.5GB/22.5GB) \n",
      "+---------+-----+-------------------+-------------------+-------------------+\n",
      "|symbol   |label|rf_prob            |gbt_prob           |lr_prob            |\n",
      "+---------+-----+-------------------+-------------------+-------------------+\n",
      "|AVA-BTC  |1    |0.5483395901154571 |0.5469867542445095 |0.47643731313104853|\n",
      "|REN-USDT |1    |0.5830462372277224 |0.5995849320462714 |0.5122312076796709 |\n",
      "|OGN-USDT |0    |0.41225576048983215|0.45720485380705056|0.4600552566017114 |\n",
      "|IOST-BNB |1    |0.5282199191647802 |0.5896720450138415 |0.47209677060623034|\n",
      "|HIVE-USDT|0    |0.4730602523909986 |0.513946249952948  |0.4497767284421649 |\n",
      "|HBAR-BTC |0    |0.4466632233572381 |0.46549198068634345|0.46889375372372233|\n",
      "|ALGO-BUSD|1    |0.569660950405586  |0.5706660199117968 |0.5023324398550622 |\n",
      "|PIVX-BTC |1    |0.5449162184743646 |0.6005759400782227 |0.4875796172470571 |\n",
      "|KAVA-BNB |1    |0.4606317653395686 |0.47269276149292183|0.33216692599717246|\n",
      "|ARDR-USDT|0    |0.5340375599950917 |0.5595224588385552 |0.4788981599894614 |\n",
      "+---------+-----+-------------------+-------------------+-------------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "#walk-forward validtion\n",
    "NUM_FOLDS = 3\n",
    "\n",
    "#since data is already chronologically ordered per symbol from feature generation, we can use a simple sequential assignment\n",
    "from pyspark.sql.functions import monotonically_increasing_id, ntile, count\n",
    "\n",
    "# Use ntile to split into NUM_FOLDS equal groups, ntile(3) over entire dataset â†’ assigns 0, 1, 2 sequentially\n",
    "df_train_features = df_train_features.withColumn(\n",
    "    \"fold\",\n",
    "    ntile(NUM_FOLDS).over(Window.orderBy(\"open_time\")) - 1\n",
    ")\n",
    "print(\"\\n=== Fold Temporal Validation ===\")\n",
    "df_train_features.groupBy(\"fold\").agg(\n",
    "    _min(\"open_time\").alias(\"start_date\"),\n",
    "    _max(\"open_time\").alias(\"end_date\"),\n",
    "    count(\"*\").alias(\"count\")\n",
    ").orderBy(\"fold\").show()\n",
    "\n",
    "#adding row_id for joins\n",
    "df_train_features = df_train_features.withColumn(\n",
    "    \"row_id\",\n",
    "    monotonically_increasing_id()\n",
    ")\n",
    "\n",
    "#checkpoint to break lineage and prevent crashes\n",
    "df_train_features = df_train_features.checkpoint()\n",
    "gc.collect()\n",
    "\n",
    "total_count = df_train_features.count()\n",
    "\n",
    "\n",
    "#initializing empty DataFrames for out-of-fold predictions\n",
    "oof_predictions = None\n",
    "\n",
    "#cross validation loop\n",
    "for fold_num in range(NUM_FOLDS):\n",
    "    \n",
    "    #skipping fold 0\n",
    "    if fold_num == 0:\n",
    "        print(f\"\\nSkipping Fold {fold_num + 1} (earliest data - need for training)\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"Fold {fold_num + 1}/{NUM_FOLDS}\")\n",
    "    \n",
    "    #training on past folds only\n",
    "    train_fold = df_train_features.filter(col(\"fold\") < fold_num)\n",
    "    valid_fold = df_train_features.filter(col(\"fold\") == fold_num)\n",
    "    \n",
    "    #materializing the splits\n",
    "    train_fold = train_fold.checkpoint()\n",
    "    valid_fold = valid_fold.checkpoint()\n",
    "    gc.collect()\n",
    "\n",
    "    #random forest\n",
    "    start_time = time.time()\n",
    "    \n",
    "    rf = RandomForestClassifier(\n",
    "        featuresCol=\"features\",\n",
    "        labelCol=\"label\",\n",
    "        weightCol=\"weight\" if use_weights else None,\n",
    "        numTrees=50,\n",
    "        maxDepth=6,\n",
    "        maxBins=24,\n",
    "        subsamplingRate=0.7,\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    rf_fold_model = rf.fit(train_fold.select(\"features\", \"label\", \"weight\") if use_weights \n",
    "                           else train_fold.select(\"features\", \"label\"))\n",
    "    \n",
    "    print(f\"RF trained in {(time.time()-start_time)/60:.1f} mins\")\n",
    "    \n",
    "    rf_valid_pred = rf_fold_model.transform(valid_fold)\n",
    "    rf_valid_pred = rf_valid_pred.withColumn(\"rf_prob\", extract_prob_udf(col(\"probability\")))\n",
    "    rf_valid_pred = rf_valid_pred.select(\"row_id\", \"rf_prob\").checkpoint()\n",
    "    gc.collect()\n",
    "    \n",
    "    #gradient boosted trees\n",
    "    start_time = time.time()\n",
    "    \n",
    "    gbt = GBTClassifier(\n",
    "        featuresCol=\"features\",\n",
    "        labelCol=\"label\",\n",
    "        maxIter=50,\n",
    "        maxDepth=5,\n",
    "        stepSize=0.1,\n",
    "        subsamplingRate=0.8,\n",
    "        seed=42\n",
    "    )\n",
    "    if ENABLE_GBT_UNDERSAMPLING:\n",
    "        train_up = train_fold.filter(col(\"label\") == 1)\n",
    "        train_down = train_fold.filter(col(\"label\") == 0)\n",
    "        up_count = train_up.count()\n",
    "        down_count = train_down.count()\n",
    "        if down_count > up_count:\n",
    "            # DOWN is majority, sample it down\n",
    "            sample_ratio = up_count / down_count\n",
    "            train_down_sampled = train_down.sample(False, sample_ratio, seed=42)\n",
    "            train_balanced = train_up.union(train_down_sampled)\n",
    "        else:\n",
    "            # UP is majority, sample it down\n",
    "            sample_ratio = down_count / up_count\n",
    "            train_up_sampled = train_up.sample(False, sample_ratio, seed=42)\n",
    "            train_balanced = train_down.union(train_up_sampled)\n",
    "\n",
    "        gbt_fold_model = gbt.fit(train_balanced.select(\"features\", \"label\"))\n",
    "    else:\n",
    "        gbt_fold_model = gbt.fit(train_fold.select(\"features\", \"label\"))\n",
    "    \n",
    "    print(f\"GBT trained in {(time.time()-start_time)/60:.1f} mins\")\n",
    "    \n",
    "    gbt_valid_pred = gbt_fold_model.transform(valid_fold)\n",
    "    gbt_valid_pred = gbt_valid_pred.withColumn(\"gbt_prob\", extract_prob_udf(col(\"probability\")))\n",
    "    gbt_valid_pred = gbt_valid_pred.select(\"row_id\", \"gbt_prob\").checkpoint()\n",
    "    gc.collect()\n",
    "    \n",
    "    #logistic regression\n",
    "    start_time = time.time()\n",
    "    \n",
    "    lr = LogisticRegression(\n",
    "        featuresCol=\"features\",\n",
    "        labelCol=\"label\",\n",
    "        weightCol=\"weight\" if use_weights else None,\n",
    "        maxIter=100,\n",
    "        regParam=0.01,\n",
    "        elasticNetParam=0.0\n",
    "    )\n",
    "    \n",
    "    lr_fold_model = lr.fit(train_fold.select(\"features\", \"label\", \"weight\") if use_weights\n",
    "                           else train_fold.select(\"features\", \"label\"))\n",
    "    \n",
    "    print(f\"LR trained in{(time.time()-start_time)/60:.1f} mins\")\n",
    "    \n",
    "    lr_valid_pred = lr_fold_model.transform(valid_fold)\n",
    "    lr_valid_pred = lr_valid_pred.withColumn(\"lr_prob\", extract_prob_udf(col(\"probability\")))\n",
    "    # Select only needed columns and checkpoint\n",
    "    lr_valid_pred = lr_valid_pred.select(\"row_id\", \"lr_prob\").checkpoint()\n",
    "    gc.collect()\n",
    "    \n",
    "    #combining predictions for this fold\n",
    "    if use_weights:\n",
    "        fold_combined = valid_fold.select(\"row_id\", \"symbol\", \"features\", \"label\", \"weight\")\n",
    "    else:\n",
    "        fold_combined = valid_fold.select(\"row_id\", \"symbol\", \"features\", \"label\")\n",
    "    \n",
    "    fold_combined = fold_combined.join(rf_valid_pred, on=\"row_id\", how=\"inner\")\n",
    "    fold_combined = fold_combined.join(gbt_valid_pred, on=\"row_id\", how=\"inner\")\n",
    "    fold_combined = fold_combined.join(lr_valid_pred, on=\"row_id\", how=\"inner\")\n",
    "    \n",
    "    #Checkpointing combined\n",
    "    fold_combined = fold_combined.checkpoint()\n",
    "    gc.collect()\n",
    "    \n",
    "    #appending to out of fold predictions\n",
    "    if oof_predictions is None:\n",
    "        oof_predictions = fold_combined\n",
    "    else:\n",
    "        oof_predictions = oof_predictions.union(fold_combined)\n",
    "    \n",
    "    #checkpoint after union\n",
    "    oof_predictions = oof_predictions.checkpoint()\n",
    "    \n",
    "    print(f\"Fold {fold_num} complete\")\n",
    "    check_memory()\n",
    "    gc.collect()\n",
    "#verifying we have all probabilities\n",
    "oof_predictions.select(\"symbol\", \"label\", \"rf_prob\", \"gbt_prob\", \"lr_prob\").show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trains the final versions of all three base models on the complete training dataset, using the same hyperparameters. These final models will generate predictions on the base set and serve as the base for the stacking ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training final base models on full data\n",
      "Training final RandomForest...\n",
      "RF trained in 37.6 min\n",
      "Training final GBT.\n",
      "GBT trained in 124.4 min\n",
      "Training final LogReg.\n",
      "LR trained in 11.3 min\n"
     ]
    }
   ],
   "source": [
    "print(\"Training final base models on full data\")\n",
    "\n",
    "# These will be used for test predictions\n",
    "print(\"Training final RandomForest.\")\n",
    "start_time = time.time()\n",
    "\n",
    "rf_final = RandomForestClassifier(\n",
    "    featuresCol=\"features\", labelCol=\"label\",\n",
    "    weightCol=\"weight\" if use_weights else None,\n",
    "    numTrees=50, maxDepth=6, maxBins=24,\n",
    "    subsamplingRate=0.7, seed=42\n",
    ")\n",
    "rf_model = rf_final.fit(df_train_features.select(\"features\", \"label\", \"weight\") if use_weights\n",
    "                        else df_train_features.select(\"features\", \"label\"))\n",
    "\n",
    "print(f\"RF trained in {(time.time()-start_time)/60:.1f} min\")\n",
    "\n",
    "print(\"Training final GBT.\")\n",
    "start_time = time.time()\n",
    "\n",
    "gbt_final = GBTClassifier(\n",
    "    featuresCol=\"features\", labelCol=\"label\",\n",
    "    maxIter=50, maxDepth=5, stepSize=0.1,\n",
    "    subsamplingRate=0.8, seed=42\n",
    ")\n",
    "if ENABLE_GBT_UNDERSAMPLING:\n",
    "    train_up = df_train_features.filter(col(\"label\") == 1)\n",
    "    train_down = df_train_features.filter(col(\"label\") == 0)\n",
    "    \n",
    "    up_count = train_up.count()\n",
    "    down_count = train_down.count()\n",
    "    \n",
    "    if down_count > up_count:\n",
    "        sample_ratio = up_count / down_count\n",
    "        train_down_sampled = train_down.sample(False, sample_ratio, seed=42)\n",
    "        train_balanced = train_up.union(train_down_sampled)\n",
    "    else:\n",
    "        sample_ratio = down_count / up_count\n",
    "        train_up_sampled = train_up.sample(False, sample_ratio, seed=42)\n",
    "        train_balanced = train_down.union(train_up_sampled)\n",
    "    \n",
    "    balanced_count = train_balanced.count()\n",
    "    gbt_model = gbt_final.fit(train_balanced.select(\"features\", \"label\"))\n",
    "else:\n",
    "    gbt_model = gbt_final.fit(df_train_features.select(\"features\", \"label\"))\n",
    "\n",
    "\n",
    "print(f\"GBT trained in {(time.time()-start_time)/60:.1f} min\")\n",
    "\n",
    "print(\"Training final LogReg.\")\n",
    "start_time = time.time()\n",
    "\n",
    "lr_final = LogisticRegression(\n",
    "    featuresCol=\"features\", labelCol=\"label\",\n",
    "    weightCol=\"weight\" if use_weights else None,\n",
    "    maxIter=100, regParam=0.01, elasticNetParam=0.0\n",
    ")\n",
    "lr_model = lr_final.fit(df_train_features.select(\"features\", \"label\", \"weight\") if use_weights\n",
    "                        else df_train_features.select(\"features\", \"label\"))\n",
    "\n",
    "print(f\"LR trained in {(time.time()-start_time)/60:.1f} min\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracts and analyzes feature importances from the tree-based models (Random Forest and Gradient Boosted Trees) to understand which technical indicators drive predictions. Logistic regression was not included here, since it's only used to add variety for the meta-learner and doesn't generate meaningful results worth analyzing. The analysis aggregates importance scores across both models by averaging their individual contributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Forest - Top 20 importance:\n",
      "Rank  Feature                                  Importance   Cumulative %\n",
      "1     body                                     0.224023      22.40%\n",
      "2     range                                    0.130031      35.41%\n",
      "3     upper_wick                               0.113722      46.78%\n",
      "4     price_to_sma5                            0.094474      56.22%\n",
      "5     atr_10                                   0.068947      63.12%\n",
      "6     price_to_sma10                           0.064193      69.54%\n",
      "7     high_lag4                                0.038484      73.39%\n",
      "8     price_momentum                           0.035905      76.98%\n",
      "9     high_lag3                                0.026647      79.64%\n",
      "10    number_of_trades_lag1                    0.026561      82.30%\n",
      "11    bb_position                              0.022108      84.51%\n",
      "12    open_lag5                                0.019942      86.50%\n",
      "13    high_lag6                                0.019068      88.41%\n",
      "14    high_lag5                                0.018697      90.28%\n",
      "15    price_to_sma20                           0.016836      91.96%\n",
      "16    number_of_trades_lag3                    0.012170      93.18%\n",
      "17    number_of_trades_lag2                    0.012059      94.39%\n",
      "18    taker_buy_quote_asset_volume_lag1        0.011418      95.53%\n",
      "19    lower_wick                               0.009380      96.47%\n",
      "20    taker_buy_quote_asset_volume_lag2        0.008037      97.27%\n",
      "\n",
      "Gradient Boosted Trees - Top 20 importance:\n",
      "Rank  Feature                                  Importance   Cumulative %\n",
      "1     body                                     0.154681      15.47%\n",
      "2     range                                    0.154338      30.90%\n",
      "3     price_to_sma5                            0.109544      41.86%\n",
      "4     upper_wick                               0.096110      51.47%\n",
      "5     lower_wick                               0.093155      60.78%\n",
      "6     atr_10                                   0.044468      65.23%\n",
      "7     price_to_sma20                           0.036436      68.87%\n",
      "8     number_of_trades_lag3                    0.034058      72.28%\n",
      "9     number_of_trades_lag1                    0.032825      75.56%\n",
      "10    number_of_trades_lag2                    0.032487      78.81%\n",
      "11    bb_position                              0.027453      81.56%\n",
      "12    open_lag6                                0.027166      84.27%\n",
      "13    price_to_sma10                           0.025582      86.83%\n",
      "14    taker_buy_quote_asset_volume_lag2        0.020824      88.91%\n",
      "15    taker_buy_quote_asset_volume_lag1        0.020131      90.93%\n",
      "16    taker_buy_quote_asset_volume_lag3        0.016851      92.61%\n",
      "17    high_lag1                                0.016839      94.29%\n",
      "18    price_momentum                           0.011584      95.45%\n",
      "19    open_lag3                                0.011478      96.60%\n",
      "20    high_lag2                                0.009718      97.57%\n",
      "\n",
      "Rank  Feature                                  Avg Importance  RF           GBT\n",
      "1     body                                     0.189352        0.224023     0.154681\n",
      "2     range                                    0.142185        0.130031     0.154338\n",
      "3     upper_wick                               0.104916        0.113722     0.096110\n",
      "4     price_to_sma5                            0.102009        0.094474     0.109544\n",
      "5     atr_10                                   0.056708        0.068947     0.044468\n",
      "6     lower_wick                               0.051267        0.009380     0.093155\n",
      "7     price_to_sma10                           0.044887        0.064193     0.025582\n",
      "8     number_of_trades_lag1                    0.029693        0.026561     0.032825\n",
      "9     price_to_sma20                           0.026636        0.016836     0.036436\n",
      "10    bb_position                              0.024781        0.022108     0.027453\n",
      "11    price_momentum                           0.023744        0.035905     0.011584\n",
      "12    high_lag4                                0.023160        0.038484     0.007837\n",
      "13    number_of_trades_lag3                    0.023114        0.012170     0.034058\n",
      "14    number_of_trades_lag2                    0.022273        0.012059     0.032487\n",
      "15    open_lag6                                0.015880        0.004593     0.027166\n",
      "16    taker_buy_quote_asset_volume_lag1        0.015774        0.011418     0.020131\n",
      "17    taker_buy_quote_asset_volume_lag2        0.014430        0.008037     0.020824\n",
      "18    high_lag3                                0.013390        0.026647     0.000133\n",
      "19    high_lag1                                0.012337        0.007836     0.016839\n",
      "20    high_lag6                                0.012145        0.019068     0.005222\n",
      "21    open_lag5                                0.011432        0.019942     0.002923\n",
      "22    taker_buy_quote_asset_volume_lag3        0.010895        0.004940     0.016851\n",
      "23    high_lag5                                0.009634        0.018697     0.000571\n",
      "24    open_lag3                                0.005774        0.000070     0.011478\n",
      "25    high_lag2                                0.005446        0.001173     0.009718\n",
      "low importance features\n",
      "\n",
      "Features with <1.0% avg importance:\n",
      "  high_lag5                                RF: 0.0187, GBT: 0.0006, Avg: 0.0096\n",
      "  open_lag3                                RF: 0.0001, GBT: 0.0115, Avg: 0.0058\n",
      "  high_lag2                                RF: 0.0012, GBT: 0.0097, Avg: 0.0054\n",
      "  close_lag4                               RF: 0.0064, GBT: 0.0042, Avg: 0.0053\n",
      "  close_lag3                               RF: 0.0023, GBT: 0.0034, Avg: 0.0028\n",
      "\n",
      "Total: 5 features\n"
     ]
    }
   ],
   "source": [
    "#extracting feature importances from tree based models\n",
    "def get_feature_importance(model, feature_names, model_name):\n",
    "    try:\n",
    "        importances = model.featureImportances.toArray()\n",
    "        \n",
    "        #creating a list of (feature_name, importance)\n",
    "        feature_importance = list(zip(feature_names, importances))\n",
    "        \n",
    "        #sorting by importance\n",
    "        feature_importance.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        print(f\"\\n{model_name} Top 20 importance:\")\n",
    "        print(f\"{'Rank':<5} {'Feature':<40} {'Importance':<12} {'Cumulative %'}\")\n",
    "        \n",
    "        cumulative = 0\n",
    "        for rank, (feature, importance) in enumerate(feature_importance[:20], 1):\n",
    "            cumulative += importance\n",
    "            print(f\"{rank:<5} {feature:<40} {importance:<12.6f} {cumulative*100:>6.2f}%\")\n",
    "        \n",
    "        return feature_importance\n",
    "    \n",
    "    except AttributeError:\n",
    "        return None\n",
    "\n",
    "#getting importances\n",
    "rf_importance = get_feature_importance(rf_model, SELECTED_FEATURES, \"Random Forest\")\n",
    "gbt_importance = get_feature_importance(gbt_model, SELECTED_FEATURES, \"Gradient Boosted Trees\")\n",
    "\n",
    "#aggregating importance across models\n",
    "if rf_importance and gbt_importance:\n",
    "    #creating a dict for averaging\n",
    "    importance_dict = {}\n",
    "    \n",
    "    for feature, imp in rf_importance:\n",
    "        importance_dict[feature] = [imp, 0]\n",
    "    \n",
    "    for feature, imp in gbt_importance:\n",
    "        if feature in importance_dict:\n",
    "            importance_dict[feature][1] = imp\n",
    "        else:\n",
    "            importance_dict[feature] = [0, imp]\n",
    "    \n",
    "    #calculating averages\n",
    "    avg_importance = [(feature, (imps[0] + imps[1]) / 2) \n",
    "                      for feature, imps in importance_dict.items()]\n",
    "    \n",
    "    #sorting by average importance\n",
    "    avg_importance.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(f\"\\n{'Rank':<5} {'Feature':<40} {'Avg Importance':<15} {'RF':<12} {'GBT'}\")\n",
    "    \n",
    "    for rank, (feature, avg_imp) in enumerate(avg_importance[:25], 1):\n",
    "        rf_imp = importance_dict[feature][0]\n",
    "        gbt_imp = importance_dict[feature][1]\n",
    "        print(f\"{rank:<5} {feature:<40} {avg_imp:<15.6f} {rf_imp:<12.6f} {gbt_imp:.6f}\")\n",
    "# After feature importance analysis, add:\n",
    "\n",
    "print(\"low importance features\")\n",
    "low_importance_threshold = 0.01 \n",
    "\n",
    "low_importance = [feat for feat, imp in avg_importance if imp < low_importance_threshold]\n",
    "\n",
    "print(f\"\\nFeatures with <{low_importance_threshold*100}% avg importance:\")\n",
    "for feat in low_importance:\n",
    "    rf_imp = importance_dict[feat][0]\n",
    "    gbt_imp = importance_dict[feat][1]\n",
    "    avg_imp = (rf_imp + gbt_imp) / 2\n",
    "    print(f\"  {feat:<40} RF: {rf_imp:.4f}, GBT: {gbt_imp:.4f}, Avg: {avg_imp:.4f}\")\n",
    "\n",
    "print(f\"\\nTotal: {len(low_importance)} features\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating base models on test set\n",
      "RandomForest AUC: 0.5927\n",
      "GradientBoostedTrees AUC: 0.5989\n",
      "LogisticRegression AUC: 0.5336\n"
     ]
    }
   ],
   "source": [
    "#getting test predictions\n",
    "rf_pred_test = rf_model.transform(df_test_features).withColumn(\"rf_prob\", extract_prob_udf(col(\"probability\")))\n",
    "gbt_pred_test = gbt_model.transform(df_test_features).withColumn(\"gbt_prob\", extract_prob_udf(col(\"probability\")))\n",
    "lr_pred_test = lr_model.transform(df_test_features).withColumn(\"lr_prob\", extract_prob_udf(col(\"probability\")))\n",
    "\n",
    "#evaluating individual base models on test set\n",
    "print(\"Evaluating base models on test set\")\n",
    "auc_evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "\n",
    "rf_auc = auc_evaluator.evaluate(rf_pred_test)\n",
    "gbt_auc = auc_evaluator.evaluate(gbt_pred_test)\n",
    "lr_auc = auc_evaluator.evaluate(lr_pred_test)\n",
    "\n",
    "print(f\"RandomForest AUC: {rf_auc:.4f}\")\n",
    "print(f\"GradientBoostedTrees AUC: {gbt_auc:.4f}\")\n",
    "print(f\"LogisticRegression AUC: {lr_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trains a meta-learner that combines the three base model predictions optimally. Uses VectorAssembler to create meta-features from the three base model probabilities (rf_prob, gbt_prob, lr_prob), treating these probabilities as input features. The meta-model is a LogisticRegression with light regularization (0.001) to prevent overfitting while learning combination weights. Trains on out-of-fold predictions from the cross-validation loop, which are unbiased since each prediction was made by a model that never saw that data during training. This prevents the meta-model from simply memorizing training set patterns. The meta-model learns when to trust each base model based on their probability patterns: for example, it might learn that RandomForest is more reliable in volatile conditions while GradientBoostedTrees performs better in trending markets. Applies the same meta-feature assembly to test predictions so the meta-model can generate final combined predictions. The trained meta-model is saved alongside the three base models for use on the real-time stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-model trained in 0.6 minutes\n",
      "Memory: 67.8% used (11.2GB/16.5GB)\n",
      "Swap: 32.6% used (7.3GB/22.5GB) \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1249"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine test predictions\n",
    "meta_test = rf_pred_test.select(\"symbol\", \"features\", \"label\", \"rf_prob\") \\\n",
    "                        .join(gbt_pred_test.select(\"symbol\", \"features\", \"gbt_prob\"), \n",
    "                              on=[\"symbol\", \"features\"], how=\"inner\") \\\n",
    "                        .join(lr_pred_test.select(\"symbol\", \"features\", \"lr_prob\"), \n",
    "                              on=[\"symbol\", \"features\"], how=\"inner\")\n",
    "\n",
    "#training the meta model on oof predictions\n",
    "#assembling meta features\n",
    "meta_assembler = VectorAssembler(\n",
    "    inputCols=[\"rf_prob\", \"gbt_prob\", \"lr_prob\"],\n",
    "    outputCol=\"meta_features\",\n",
    "    handleInvalid=\"skip\"\n",
    ")\n",
    "\n",
    "meta_train = meta_assembler.transform(oof_predictions)\n",
    "meta_test = meta_assembler.transform(meta_test)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "meta_model = LogisticRegression(\n",
    "    featuresCol=\"meta_features\",\n",
    "    labelCol=\"label\",\n",
    "    weightCol=\"weight\" if use_weights else None,\n",
    "    maxIter=100,\n",
    "    regParam=0.001\n",
    ")\n",
    "\n",
    "stacked_model = meta_model.fit(meta_train.select(\"meta_features\", \"label\", \"weight\") if use_weights\n",
    "                               else meta_train.select(\"meta_features\", \"label\"))\n",
    "\n",
    "print(f\"Meta-model trained in {(time.time()-start_time)/60:.1f} minutes\")\n",
    "check_memory()\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluates the performance ofthe ensemble by using three metrics: accuracy (overall correctness across all predictions), AUC (measures the models ability to distinguish between classes across probability thresholds), F1 score (balancing false positives and false negatives). Generates a confusion matrix to compare predictions vs actual labels. Generates per-class recall metrics to ensure proper balancing between directions and adjust weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+----------+----------------------------------------+\n",
      "|symbol |label|prediction|probability                             |\n",
      "+-------+-----+----------+----------------------------------------+\n",
      "|SXP-EUR|1    |0.0       |[0.7172629417673959,0.2827370582326041] |\n",
      "|SXP-EUR|0    |0.0       |[0.7565512894618713,0.24344871053812867]|\n",
      "|SXP-EUR|0    |0.0       |[0.7578277359923022,0.24217226400769776]|\n",
      "|SXP-EUR|0    |0.0       |[0.8215137489170786,0.17848625108292138]|\n",
      "|SXP-EUR|0    |0.0       |[0.8178598444766144,0.18214015552338558]|\n",
      "|SXP-EUR|0    |0.0       |[0.8178598444766144,0.18214015552338558]|\n",
      "|SXP-EUR|0    |0.0       |[0.8178598444766144,0.18214015552338558]|\n",
      "|SXP-EUR|0    |0.0       |[0.8178598444766144,0.18214015552338558]|\n",
      "|SXP-EUR|0    |0.0       |[0.8178598444766144,0.18214015552338558]|\n",
      "|SXP-EUR|0    |0.0       |[0.8178598444766144,0.18214015552338558]|\n",
      "+-------+-----+----------+----------------------------------------+\n",
      "only showing top 10 rows\n",
      "Stack performance\n",
      "Accuracy: 0.5681 (56.81%)\n",
      "AUC: 0.6016\n",
      "F1: 0.5696\n",
      "+-----+----------+-------+\n",
      "|label|prediction|  count|\n",
      "+-----+----------+-------+\n",
      "|    0|       0.0|3335162|\n",
      "|    0|       1.0|2590860|\n",
      "|    1|       0.0|1996364|\n",
      "|    1|       1.0|2699229|\n",
      "+-----+----------+-------+\n",
      "\n",
      "UP recall: 0.5748 (57.48%)\n",
      "DOWN recall: 0.5628 (56.28%)\n"
     ]
    }
   ],
   "source": [
    "#evaluation\n",
    "final_predictions = stacked_model.transform(meta_test)\n",
    "final_predictions = final_predictions.checkpoint()\n",
    "gc.collect()\n",
    "final_predictions.select(\"symbol\", \"label\", \"prediction\", \"probability\").show(10, truncate=False)\n",
    "\n",
    "# Metrics\n",
    "accuracy_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "auc_evaluator_final = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "f1_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "\n",
    "stacked_accuracy = accuracy_evaluator.evaluate(final_predictions)\n",
    "stacked_auc = auc_evaluator_final.evaluate(final_predictions)\n",
    "stacked_f1 = f1_evaluator.evaluate(final_predictions)\n",
    "\n",
    "print(f\"Stack performance\")\n",
    "print(f\"Accuracy: {stacked_accuracy:.4f} ({stacked_accuracy*100:.2f}%)\")\n",
    "print(f\"AUC: {stacked_auc:.4f}\")\n",
    "print(f\"F1: {stacked_f1:.4f}\")\n",
    "\n",
    "final_predictions.groupBy(\"label\", \"prediction\").count().orderBy(\"label\", \"prediction\").show()\n",
    "\n",
    "#per-class metrics\n",
    "confusion = final_predictions.groupBy(\"label\", \"prediction\").count().collect()\n",
    "confusion_dict = {(row['label'], row['prediction']): row['count'] for row in confusion}\n",
    "\n",
    "total_up = confusion_dict.get((1, 0), 0) + confusion_dict.get((1, 1), 0)\n",
    "correct_up = confusion_dict.get((1, 1), 0)\n",
    "total_down = confusion_dict.get((0, 0), 0) + confusion_dict.get((0, 1), 0)\n",
    "correct_down = confusion_dict.get((0, 0), 0)\n",
    "\n",
    "up_recall = correct_up / total_up if total_up > 0 else 0\n",
    "down_recall = correct_down / total_down if total_down > 0 else 0\n",
    "\n",
    "print(f\"UP recall: {up_recall:.4f} ({up_recall*100:.2f}%)\")\n",
    "print(f\"DOWN recall: {down_recall:.4f} ({down_recall*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Assesses if the model's predicted probabilities match true likelyhood. Groups predictions into 5% probability bins and calculates two metrics per bin: the average predicted probability and the actual success rate (percentage that were truly UP).  Calculates mean absolute calibration error by averaging the absolute differences across all bins, weighted by the number of predictions in each bin. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibration table:\n",
      "+--------+-------+-------------------+--------------------+---------------------+\n",
      "|prob_bin|count  |avg_predicted_prob |actual_prob_up      |calibration_error    |\n",
      "+--------+-------+-------------------+--------------------+---------------------+\n",
      "|0.1     |14755  |0.14673957979679453|0.016468993561504573|-0.13027058623528995 |\n",
      "|0.15    |185014 |0.175953570121973  |0.059427935183283424|-0.11652563493868956 |\n",
      "|0.2     |101429 |0.22868296842935354|0.14135010697137898 |-0.08733286145797456 |\n",
      "|0.25    |184074 |0.27598477684267897|0.18821778197898673 |-0.08776699486369224 |\n",
      "|0.3     |306793 |0.32780809486830764|0.27037122750519077 |-0.05743686736311687 |\n",
      "|0.35    |735674 |0.3789465932833661 |0.3391542993227979  |-0.039792293960568215|\n",
      "|0.4     |982890 |0.4261534352734992 |0.3818942099319354  |-0.04425922534156379 |\n",
      "|0.45    |2820897|0.47908378748012187|0.43543879836803684 |-0.04364498911208503 |\n",
      "|0.5     |2861299|0.5269806528647341 |0.4857821569853413  |-0.04119849587939278 |\n",
      "|0.55    |2064607|0.56941185862268   |0.5323603959494471  |-0.03705146267323289 |\n",
      "|0.6     |364019 |0.6096816264180827 |0.5770001016430462  |-0.0326815247750365  |\n",
      "|0.65    |161    |0.6563422122869419 |0.6459627329192547  |-0.010379479367687239|\n",
      "|0.7     |3      |0.7006935238107183 |1.0                 |0.29930647618928174  |\n",
      "+--------+-------+-------------------+--------------------+---------------------+\n",
      "\n",
      "Mean absolute calibration error: 0.0441\n"
     ]
    }
   ],
   "source": [
    "#calibration analysis, checking if predicted probabilities match actual outcomes\n",
    "\n",
    "#extracting UP probability from final predictions\n",
    "final_predictions = final_predictions.withColumn(\"prob_up\", extract_prob_udf(col(\"probability\")))\n",
    "\n",
    "#creating probability bins (5% width)\n",
    "from pyspark.sql.functions import floor as spark_floor, round as spark_round, avg, count\n",
    "\n",
    "final_predictions_binned = final_predictions.withColumn(\n",
    "    \"prob_bin\",\n",
    "    spark_round(spark_floor(col(\"prob_up\") * 20) / 20, 2)\n",
    ")\n",
    "\n",
    "#calculating calibration metrics per bin\n",
    "calibration = final_predictions_binned.groupBy(\"prob_bin\").agg(\n",
    "    count(\"*\").alias(\"count\"),\n",
    "    avg(\"prob_up\").alias(\"avg_predicted_prob\"),\n",
    "    avg(when(col(\"label\") == 1, 1.0).otherwise(0.0)).alias(\"actual_prob_up\")\n",
    ")\n",
    "\n",
    "#adding calibration error\n",
    "calibration = calibration.withColumn(\n",
    "    \"calibration_error\",\n",
    "    col(\"actual_prob_up\") - col(\"avg_predicted_prob\")\n",
    ")\n",
    "\n",
    "print(\"Calibration table:\")\n",
    "calibration.orderBy(\"prob_bin\").show(25, truncate=False)\n",
    "\n",
    "#overall calibration metrics\n",
    "calibration_data = calibration.collect()\n",
    "total_samples = sum(row['count'] for row in calibration_data)\n",
    "weighted_cal_error = sum(row['count'] * abs(row['calibration_error']) for row in calibration_data) / total_samples\n",
    "#calibration error <5% - near perfect\n",
    "print(f\"Mean absolute calibration error: {weighted_cal_error:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzes the distribution of predicted probabilities and evaluates model performance across different confidence levels. Predictions are categorized into confidence ranges, the distribution shows how often predictions are made in each confidence tier. High confidence subset analysis filters predictions where probability is either â‰¥60% or â‰¤40% showing where the model has clear directional confidence, for example. Accuracy is calculated separately for subsets and compared against overall model performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability distribution:\n",
      "+-------+-------------------+\n",
      "|summary|            prob_up|\n",
      "+-------+-------------------+\n",
      "|  count|           10621615|\n",
      "|   mean| 0.4861669783835595|\n",
      "| stddev| 0.0872711742406609|\n",
      "|    min|0.14062491772922492|\n",
      "|    max| 0.7011341491853222|\n",
      "+-------+-------------------+\n",
      "\n",
      "Confidence distribution:\n",
      "+-------------------+-------+\n",
      "|confidence_range   |count  |\n",
      "+-------------------+-------+\n",
      "|Borderline (50-55%)|2861299|\n",
      "|Confident (60-65%) |364019 |\n",
      "|High (65-70%)      |161    |\n",
      "|Low (45-50%)       |2820897|\n",
      "|Moderate (55-60%)  |2064607|\n",
      "|Very High (70%+)   |3      |\n",
      "|Very Low (0-45%)   |2510629|\n",
      "+-------------------+-------+\n",
      "\n",
      "High Confidence (prob > 0.6 or < 0.4):\n",
      "  Predictions: 1,891,922 (17.8% of total)\n",
      "  Accuracy: 0.7110 (71.10%)\n",
      "  vs Overall: 0.5681 (56.81%)\n",
      "\n",
      "Very High Confidence (prob > 0.7 or < 0.30000000000000004):\n",
      "  Predictions: 485,275 (4.6% of total)\n",
      "  Accuracy: 0.8759 (87.59%)\n",
      "  vs Overall: 0.5681 (56.81%)\n"
     ]
    }
   ],
   "source": [
    "# Probability distribution analysis\n",
    "print(\"Probability distribution:\")\n",
    "final_predictions.select(\"prob_up\").describe().show()\n",
    "\n",
    "#counting predictions by confidence ranges\n",
    "print(\"Confidence distribution:\")\n",
    "final_predictions.withColumn(\n",
    "    \"confidence_range\",\n",
    "    when(col(\"prob_up\") < 0.45, \"Very Low (0-45%)\")\n",
    "    .when(col(\"prob_up\") < 0.50, \"Low (45-50%)\")\n",
    "    .when(col(\"prob_up\") < 0.55, \"Borderline (50-55%)\")\n",
    "    .when(col(\"prob_up\") < 0.60, \"Moderate (55-60%)\")\n",
    "    .when(col(\"prob_up\") < 0.65, \"Confident (60-65%)\")\n",
    "    .when(col(\"prob_up\") < 0.70, \"High (65-70%)\")\n",
    "    .otherwise(\"Very High (70%+)\")\n",
    ").groupBy(\"confidence_range\").agg(\n",
    "    count(\"*\").alias(\"count\")\n",
    ").orderBy(\"confidence_range\").show(truncate=False)\n",
    "\n",
    "#high confidence subset performance\n",
    "#defining thresholds\n",
    "high_conf_threshold = 0.6  # 60% threshold\n",
    "very_high_conf_threshold = 0.7  # 70% threshold\n",
    "\n",
    "#high confidence\n",
    "high_conf_predictions = final_predictions.filter(\n",
    "    (col(\"prob_up\") >= high_conf_threshold) | (col(\"prob_up\") <= (1 - high_conf_threshold))\n",
    ")\n",
    "high_conf_count = high_conf_predictions.count()\n",
    "total_count = final_predictions.count()\n",
    "high_conf_pct = (high_conf_count / total_count * 100) if total_count > 0 else 0\n",
    "\n",
    "if high_conf_count > 0:\n",
    "    high_conf_correct = high_conf_predictions.filter(col(\"prediction\") == col(\"label\")).count()\n",
    "    high_conf_acc = high_conf_correct / high_conf_count\n",
    "    \n",
    "    print(f\"High Confidence (prob > {high_conf_threshold} or < {1-high_conf_threshold}):\")\n",
    "    print(f\"  Predictions: {high_conf_count:,} ({high_conf_pct:.1f}% of total)\")\n",
    "    print(f\"  Accuracy: {high_conf_acc:.4f} ({high_conf_acc*100:.2f}%)\")\n",
    "    print(f\"  vs Overall: {stacked_accuracy:.4f} ({stacked_accuracy*100:.2f}%)\")\n",
    "else:\n",
    "    print(f\"No high confidence predictions\")\n",
    "\n",
    "#vhc predictions\n",
    "very_high_conf_predictions = final_predictions.filter(\n",
    "    (col(\"prob_up\") >= very_high_conf_threshold) | (col(\"prob_up\") <= (1 - very_high_conf_threshold))\n",
    ")\n",
    "\n",
    "very_high_conf_count = very_high_conf_predictions.count()\n",
    "very_high_conf_pct = (very_high_conf_count / total_count * 100) if total_count > 0 else 0\n",
    "\n",
    "if very_high_conf_count > 0:\n",
    "    very_high_conf_correct = very_high_conf_predictions.filter(col(\"prediction\") == col(\"label\")).count()\n",
    "    very_high_conf_acc = very_high_conf_correct / very_high_conf_count\n",
    "    \n",
    "    print(f\"\\nVery High Confidence (prob > {very_high_conf_threshold} or < {1-very_high_conf_threshold}):\")\n",
    "    print(f\"  Predictions: {very_high_conf_count:,} ({very_high_conf_pct:.1f}% of total)\")\n",
    "    print(f\"  Accuracy: {very_high_conf_acc:.4f} ({very_high_conf_acc*100:.2f}%)\")\n",
    "    print(f\"  vs Overall: {stacked_accuracy:.4f} ({stacked_accuracy*100:.2f}%)\")\n",
    "else:\n",
    "    print(f\"No very high confidence predictions\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving models\n",
    "rf_path = os.path.join(MODEL_FOLDER, \"stacked_rf_model\").replace(\"\\\\\", \"/\")\n",
    "gbt_path = os.path.join(MODEL_FOLDER, \"stacked_gbt_model\").replace(\"\\\\\", \"/\")\n",
    "lr_path = os.path.join(MODEL_FOLDER, \"stacked_lr_model\").replace(\"\\\\\", \"/\")\n",
    "meta_path = os.path.join(MODEL_FOLDER, \"stacked_meta_model\").replace(\"\\\\\", \"/\")\n",
    "\n",
    "rf_model.write().overwrite().save(rf_path)\n",
    "gbt_model.write().overwrite().save(gbt_path)\n",
    "lr_model.write().overwrite().save(lr_path)\n",
    "stacked_model.write().overwrite().save(meta_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final cleanup and stopping spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up checkpoints\n"
     ]
    }
   ],
   "source": [
    "#cleanup\n",
    "print(\"Cleaning up checkpoints\")\n",
    "try:\n",
    "    final_predictions.unpersist()\n",
    "    meta_train.unpersist()\n",
    "    meta_test.unpersist()\n",
    "    df_train_features.unpersist()\n",
    "    df_test_features.unpersist()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    spark.catalog.clearCache()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "spark.stop()\n",
    "try:\n",
    "    shutil.rmtree(CHECKPOINT_DIR)\n",
    "except Exception as e:\n",
    "    print(f\"Warning: {e}\")\n",
    "\n",
    "try:\n",
    "    if os.path.exists(str(BASE_TEMP_DIR)):\n",
    "        shutil.rmtree(str(BASE_TEMP_DIR))\n",
    "        print(f\"Temp directory cleaned: {BASE_TEMP_DIR}\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not clean temp directory - {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
