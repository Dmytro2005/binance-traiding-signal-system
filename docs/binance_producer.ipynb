{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binance Producer\n",
    "\n",
    "This document explains each section of `src/binance_producer.py`.  \n",
    "The script is a WebSocket producer that connects to Binance's API and streams closed candlestick data to a Kafka topic.\n",
    "\n",
    "**Idea:** Producer automatically retrieves Bitcoin price data from the Binance exchange every 30 minutes and transmits it to a data processing system (Kafka) for further analysis.\n",
    "\n",
    "**Plan:**   \n",
    "1. Connect to Binance via WebSocket.  \n",
    "2. Receive completed 30-minute candlesticks.  \n",
    "3. Extract important data (prices, volumes etc).  \n",
    "4. Send them to Kafka for storage and further analysis.  \n",
    "5. Automatically reconnect upon errors.  \n",
    "\n",
    "---\n",
    "\n",
    "## 1. Preprocessing\n",
    "\n",
    "### 1.1 Library Imports\n",
    "\n",
    "Standard library and third-party imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import asyncio\n",
    "import websockets\n",
    "from kafka import KafkaProducer\n",
    "from datetime import datetime, timezone\n",
    "from typing import Dict, List, Optional\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each library serves a specific purpose in the data pipeline:\n",
    "\n",
    "- **json**: Binance sends data in JSON format, so we need to parse incoming messages and serialize outgoing records before sending to Kafka.\n",
    "\n",
    "- **asyncio**: Asynchronous framework, that allows the program to wait for WebSocket messages without blocking other operations. When waiting for data from Binance, the program can handle other tasks or simply wait efficiently.\n",
    "\n",
    "- **websockets**: Unlike HTTP requests, WebSocket maintains a persistent bidirectional connection. Once connected to Binance, we receive a continuous stream of price updates as they happen in real time.\n",
    "\n",
    "- **KafkaProducer**: The Kafka client for sending messages to a Kafka topic. \n",
    "\n",
    "- **datetime, timezone**: Used to timestamp each record with the exact moment it was received. \n",
    "\n",
    "- **typing.Dict, List, Optional**: Type hints for better code readability.\n",
    "\n",
    "- **logging**: Logging framework for diagnostics.\n",
    "\n",
    "---\n",
    "\n",
    "### 1.2 Logging configuration\n",
    "\n",
    "Sets up the root logger so that all log messages show timestamp when the log entry was created, Name of the logger, severity level (INFO, WARNING, ERROR, etc.), and message.    \n",
    "The module-level logger is used throughout the producer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reasons why we use logging:  \n",
    "\n",
    "- **Troubleshooting**: When something goes wrong, timestamps help identify when the issue started.  \n",
    "\n",
    "- **Monitoring**: It creates dashboards and alerts.\n",
    "\n",
    "- **Debugging**: We can see the exact sequence of events leading to an error. \n",
    "\n",
    "---\n",
    "\n",
    "## 2. BinanceKlineProducer Class\n",
    "\n",
    "The core producer component that bridges Binance's real-time market data with Kafka infrastructure. It manages WebSocket connections, message parsing, data validation, and reliable message delivery with built-in error recovery.\n",
    "\n",
    "### 2.1 Core Parameters\n",
    "\n",
    "The `__init__` method initializes the producer with configuration needed to connect to both Binance and Kafka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(\n",
    "    self,\n",
    "    symbols: List[str],\n",
    "    kafka_topic: str = \"binance_kline\",\n",
    "    kafka_bootstrap_servers: str = \"localhost:9092\",\n",
    "    interval: str = \"30m\"\n",
    "):\n",
    "    self.symbols = [s.lower() for s in symbols]\n",
    "    self.kafka_topic = kafka_topic\n",
    "    self.kafka_bootstrap_servers = kafka_bootstrap_servers\n",
    "    self.interval = interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each parameter has a specific purpose:\n",
    "\n",
    "- **symbols: List[str]**: Trading pairs to monitor, like [\"BTCUSDT\", \"ETHUSDT\"].\n",
    "\n",
    "- **kafka_topic: str = \"binance_kline\"**: The Kafka topic name where messages will be published and from where consumer will then read candelstick data.\n",
    "\n",
    "- **kafka_bootstrap_servers: str = \"localhost:9092\"**: The Kafka broker address.\n",
    "\n",
    "- **interval: str = \"30m\"**: Candlestick time interval, so each candlestick aggregates all trades within this period. \n",
    "\n",
    "All symbols are normalized to lowercase for the Binance stream URL.\n",
    "\n",
    "---\n",
    "\n",
    "### 2.2 Interval Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interval not in [\"1m\", \"30m\"]:\n",
    "        raise ValueError(\"Interval must be '1m' or '30m'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project supports 1m and 30m intervals from Binance. We use native intervals directly rather than manually aggregating data, which simplifies the code and guarantees proper candle alignment.     \n",
    "The code also supports 1 minute for potential future use cases, but the current pipeline is optimized for 30-minute candles.\n",
    "\n",
    "---\n",
    "\n",
    "### 2.3 WebSocket URL and Kafka Producer Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streams = \"/\".join([f\"{symbol}@kline_{interval}\" for symbol in self.symbols])\n",
    "    self.ws_url = f\"wss://stream.binance.com/stream?streams={streams}\"\n",
    "\n",
    "    self.producer = KafkaProducer(\n",
    "        bootstrap_servers=kafka_bootstrap_servers,\n",
    "        value_serializer=lambda v: json.dumps(v).encode('utf-8'),\n",
    "        acks='all',\n",
    "        retries=3\n",
    "    )\n",
    "\n",
    "    logger.info(f\"Initialized producer for {len(symbols)} symbols: {', '.join(self.symbols)}\")\n",
    "    logger.info(f\"WebSocket URL: {self.ws_url}\")\n",
    "    logger.info(f\"Kafka topic: {kafka_topic}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binance allows subscribing to multiple streams simultaneously through a single WebSocket connection.    \n",
    "\n",
    "Each parameter in producer configures how messages are sent to Kafka:\n",
    "\n",
    "- **bootstrap_servers**: Initial connection point to the Kafka cluster.\n",
    "\n",
    "- **value_serializer**: Automatically converts Python data to JSON and encodes it as bytes. Kafka requires all messages to be in byte format, so this function handles the serialization transparently before each send operation.\n",
    "\n",
    "- **acks='all'**: Acknowledgment level for durability. This ensures every message is replicated to multiple brokers before confirming success, because losing financial data is unacceptable.\n",
    "\n",
    "- **retries=3**: Retries up to 3 times, if a send fails.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Message parsing\n",
    "\n",
    "Parses a WebSocket message from Binance and returns a clean, structured record ready for storage and analysis, only for **closed** candles.\n",
    "\n",
    "### 3.1 Extracting Stream Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_kline(self, message: Dict) -> Optional[Dict]:\n",
    "    try:\n",
    "        data = message.get(\"data\", message)\n",
    "        k = data.get(\"k\")\n",
    "        \n",
    "        if not k:\n",
    "            return None\n",
    "            \n",
    "        if not k.get(\"x\"):\n",
    "                return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using Binance multiple symbols in one connection, the API wraps the actual candlestick data inside a **\"data\"** key.        \n",
    "The actual candlestick information is always nested within the **\"k\"** (kline) key, which contains all the price data.         \n",
    "If there's no \"k\" field, this isn't a candlestick message, so it will return \"None\" to skip processing.     \n",
    "**Important**: We only process **closed** candles (\"x\": true). Open candles are constantly updating and aren't suitable for analysis.     \n",
    "\n",
    "---\n",
    "\n",
    "### 3.2 Building the record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record = {\n",
    "            \"open_time\": k[\"t\"],\n",
    "            \"open\": float(k[\"o\"]),\n",
    "            \"high\": float(k[\"h\"]),\n",
    "            \"low\": float(k[\"l\"]),\n",
    "            \"close\": float(k[\"c\"]),\n",
    "            \"volume\": float(k[\"v\"]),\n",
    "            \"quote_asset_volume\": float(k[\"q\"]),\n",
    "            \"number_of_trades\": int(k[\"n\"]),\n",
    "            \"taker_buy_base_asset_volume\": float(k[\"V\"]),\n",
    "            \"taker_buy_quote_asset_volume\": float(k[\"Q\"]),\n",
    "            \"symbol\": data.get(\"s\", \"\").upper(),\n",
    "            \"interval\": k[\"i\"],\n",
    "            \"ingested_at\": datetime.now(timezone.utc).isoformat()\n",
    "        }\n",
    "        \n",
    "        return record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each field provides essential information for technical analysis and machine learning models:\n",
    "\n",
    "- **open_time**: Timestamp in milliseconds, which represents when the candlestick period started.\n",
    "\n",
    "- **open**: First trade price in the period.\n",
    "\n",
    "- **high**: Highest trade price reached during the period, it shows the peak enthusiasm/resistance level. \n",
    "\n",
    "- **low**: Lowest trade price during the period, it shows support level or panic selling point. \n",
    "\n",
    "- **close**: Last trade price in the period. Most important for trend analysis is to see whether close > open or close < open. \n",
    "\n",
    "- **volume**: Total base asset traded. High volume indicates strong conviction in price movement. \n",
    "\n",
    "- **quote_asset_volume**: Total e.g. USDT spent/received in trades. While volume shows  e.g. BTC amount, this shows the dollar value.\n",
    "\n",
    "- **number_of_trades**: Count of individual trades.\n",
    "\n",
    "- **taker_buy_base_asset_volume**: Python's built-in logging framework. Instead of using `print()` statements, we use structured logging which includes timestamps, severity levels, and allows filtering of log messages.\n",
    "\n",
    "- **taker_buy_quote_asset_volume**: Python's built-in logging framework. Instead of using `print()` statements, we use structured logging which includes timestamps, severity levels, and allows filtering of log messages.\n",
    "\n",
    "- **symbol**: Trading pair in uppercase. \n",
    "\n",
    "- **interval**: Confirms the candlestick interval, in our case 30 m. \n",
    "\n",
    "- **ingested_at**: Timestamp of when we received this data. \n",
    "\n",
    "---\n",
    "\n",
    "### 3.3 Error handling wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # ... parsing logic ...\n",
    "except (KeyError, ValueError, TypeError) as e:\n",
    "    logger.error(f\"Error parsing {e}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entire method is wrapped in a **try-except** block because:\n",
    "\n",
    "- **KeyError**: If Binance changes their message format or a required field is missing. \n",
    "\n",
    "- **ValueError**: If a string can't be converted to float. \n",
    "\n",
    "- **TypeError**: If we try to call a method on None or the wrong type. \n",
    "\n",
    "---\n",
    "\n",
    "## 4. Main Producer Loop\n",
    "\n",
    "Async method that runs until interrupted. It connects to the WebSocket with a 20s ping interval and 10s ping timeout to detect dead connections. \n",
    "\n",
    "**Processing flow:**\n",
    "1. Receive raw message\n",
    "2. Parse JSON\n",
    "3. Validate and transform into record\n",
    "4. Send to Kafka\n",
    "5. Log successful operation\n",
    "\n",
    "### 4.1 WebSocket Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run(self):\n",
    "    logger.info(\"Starting the producer\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            async with websockets.connect(\n",
    "                self.ws_url,\n",
    "                ping_interval=20,\n",
    "                ping_timeout=10\n",
    "            ) as ws:\n",
    "                logger.info(f\"Connected to websocket for {len(self.symbols)} symbols\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The WebSocket connection is configured with a **ping interval** of 20 seconds, meaning the client sends a ping message every 20 seconds to verify the connection is still active. If the server doesn't respond within 10 seconds (**ping timeout**), the connection is considered dead and will be automatically closed and reconnected. This mechanism ensures that any broken or unresponsive connections are detected quickly rather than waiting indefinitely for data that will never arrive.\n",
    "\n",
    "---\n",
    "\n",
    "### 4.2 Message Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candle_count = 0\n",
    "                async for raw_message in ws:\n",
    "                    message = json.loads(raw_message)\n",
    "                    record = self.parse_kline(message)\n",
    "    \n",
    "                    if record:\n",
    "                        self.producer.send(self.kafka_topic, value=record)\n",
    "                        candle_count += 1\n",
    "        \n",
    "                        logger.info(\n",
    "                            f\"Sent candle #{candle_count}: {record['symbol']} \"\n",
    "                            f\"@ {record['open_time']} | Close: {record['close']:.2f}\"\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The producer initializes a counter at zero to track the number of candlesticks processed during the session. As each raw WebSocket message arrives, it's immediately parsed from JSON format into a Python dictionary. This dictionary is then passed to the parse_kline method, which validates the data and transforms it into our standardized record format.   \n",
    "Only valid, closed candlesticks are sent to Kafka—if parse_kline returns None the loop continues without sending anything.   \n",
    "When a valid record is received, it's immediately sent to the Kafka topic. We then increment the counter using += 1 to maintain an accurate count of successfully processed candlesticks, which is useful for monitoring throughput.   \n",
    "\n",
    "---\n",
    "\n",
    "### 4.3 Automatic Reconnection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "except websockets.exceptions.ConnectionClosed as e:\n",
    "                logger.warning(f\"Connection closed: {e}, reconnecting\")\n",
    "                await asyncio.sleep(2)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Unexpected error: {e}, reconnecting\")\n",
    "                await asyncio.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We handle two types of failures:    \n",
    "1. When the WebSocket connection closes (which Binance does routinely every 24 hours) the producer logs a warning and waits 2 seconds before reconnecting.    \n",
    "2. For unexpected errors like network issues or parsing failures, it waits 5 seconds before retrying. The longer pause helps prevent rapid retry loops during more serious problems.     \n",
    "\n",
    "Both handlers sit inside the infinite loop, so the producer automatically reconnects and continues streaming without manual intervention.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Graceful Shutdown\n",
    "\n",
    "This method ensures graceful shutdown without data loss. It's critical for data integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def close(self):\n",
    "        self.producer.flush()\n",
    "        self.producer.close()\n",
    "        logger.info(\"Producer closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kafka producers use batching for efficiency—instead of sending each message individually, they accumulate messages in an internal buffer and send them in batches. This dramatically improves throughput.    \n",
    "Without flushing, any messages still in the buffer when the program exits are simply discarded. The flush() method forces the producer to send all buffered messages immediately and waits for Kafka to acknowledge receipt before continuing. This ensures that every candlestick we processed actually reaches Kafka, even the last few messages that arrived right before shutdown.    \n",
    "After flushing, the producer is properly closed to release network sockets, memory, and file descriptors. \n",
    "\n",
    "---\n",
    "\n",
    "## 6. Starting the producer\n",
    "\n",
    "This is where the producer begins its work. When the script is executed, it initializes with BTCUSDT as the trading symbol, connects to a local Kafka server at localhost:9092, and starts streaming 30-minute candlestick data to the binance_kline topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    SYMBOLS = [\"BTCUSDT\"]\n",
    "    KAFKA_TOPIC = \"binance_kline\"\n",
    "    KAFKA_SERVERS = \"localhost:9092\"\n",
    "\n",
    "    producer = BinanceKlineProducer(\n",
    "        symbols=SYMBOLS,\n",
    "        kafka_topic=KAFKA_TOPIC,\n",
    "        kafka_bootstrap_servers=KAFKA_SERVERS,\n",
    "        interval=\"30m\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        await producer.run()\n",
    "    except KeyboardInterrupt:\n",
    "        logger.info(\"Shutting down\")\n",
    "    finally:\n",
    "        producer.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The producer runs continuously until manually stopped with KeyboardInterrupt, at which point it gracefully shuts down and ensures all buffered messages are sent to Kafka before closing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
