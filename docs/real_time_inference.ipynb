{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-time inference\n",
    "\n",
    "This document explains each section of `src/real_time_inference.py`. The module consumes candles from Kafka, keeps a sliding window of 21×30m candles, generates features, runs the stacked ML models, and outputs trading signals. It also backfills the previous prediction’s actual label when the next candle arrives.\n",
    "\n",
    "## Imports and shared utils\n",
    "\n",
    "Imports for the Spark session, ML models (RF, GBT, LR), VectorAssembler, and a deque for the sliding window. The try/except imports `shared_utils` so the script works if run from the project root or from `src/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import deque\n",
    "from typing import Dict, List, Optional\n",
    "import logging\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.classification import (\n",
    "    RandomForestClassificationModel,\n",
    "    GBTClassificationModel,\n",
    "    LogisticRegressionModel)\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "try:\n",
    "    from shared_utils import generate_features, aggregate_candles, extract_prob_udf\n",
    "except ImportError:\n",
    "    from src.shared_utils import generate_features, aggregate_candles, extract_prob_udf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging and constants\n",
    "\n",
    "Logging is configured for the module. Constants define the candle timeframe (30m), lookback length (20), and window size (21 = lookback + 1). The extra candle is needed because features use lagged values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "CANDLE_MINUTES = 30\n",
    "LOOKBACK = 20\n",
    "WINDOW_SIZE = LOOKBACK + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class `CandlesProcessor`\n",
    "\n",
    "Creates the real-time processor: initializes Spark, minute buffer (for 1m aggregation, unused when using 30m), a deque of max 21 for the 30m window, prediction output path, and `pending_prediction` for backfilling. Loads the four trained models (RF, GBT, LR, meta-learner) from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CandlesProcessor:\n",
    "    def __init__(self, model_dir: str = \"models\", predictions_dir: str = \"data/live_predictions_parquet\"):\n",
    "        self.spark = self._initialize_spark()\n",
    "\n",
    "        self.minute_buffer: List[Dict] = []\n",
    "        self.window_30m = deque(maxlen=WINDOW_SIZE)\n",
    "\n",
    "        self.predictions_path = predictions_dir\n",
    "        self.pending_prediction: Optional[Dict] = None\n",
    "\n",
    "        logger.info(\"Loading trained models\")\n",
    "        self.rf_model = RandomForestClassificationModel.load(\n",
    "            os.path.join(model_dir, \"stacked_rf_model\")\n",
    "        )\n",
    "        self.gbt_model = GBTClassificationModel.load(\n",
    "            os.path.join(model_dir, \"stacked_gbt_model\")\n",
    "        )\n",
    "        self.lr_model = LogisticRegressionModel.load(\n",
    "            os.path.join(model_dir, \"stacked_lr_model\")\n",
    "        )\n",
    "        self.meta_model = LogisticRegressionModel.load(\n",
    "            os.path.join(model_dir, \"stacked_meta_model\")\n",
    "        )\n",
    "        logger.info(\"All models loaded successfully\")\n",
    "\n",
    "        os.makedirs(predictions_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## `_initialize_spark`\n",
    "\n",
    "Builds the Spark session for real-time inference: adaptive execution, 8 shuffle partitions, 2g driver/executor memory, and codegen disabled to avoid compatibility issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _initialize_spark(self) -> SparkSession:\n",
    "        return (SparkSession.builder\n",
    "            .appName(\"CryptoTradingSignals_RealTime\")\n",
    "            .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "            .config(\"spark.sql.shuffle.partitions\", \"8\")\n",
    "            .config(\"spark.driver.memory\", \"2g\")\n",
    "            .config(\"spark.executor.memory\", \"2g\")\n",
    "            .config(\"spark.sql.codegen.wholeStage\", \"false\")\n",
    "            .config(\"spark.sql.codegen.factoryMode\", \"NO_CODEGEN\")\n",
    "            .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## `add_1minute_candle` and `add_candle`\n",
    "\n",
    "`add_1minute_candle` appends a 1m candle to the buffer until 30 are collected. If collected it aggregates them into one bigger 30m candle and adds it to the rolling window. It also checks the candle’s `interval` and either adds a 30m candle directly or passes 1m candles to the buffer/aggregation path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m8\u001b[0m\n\u001b[0;31m    def add_candle(self, candle: Dict):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "def add_1minute_candle(self, candle: Dict):\n",
    "        self.minute_buffer.append(candle)\n",
    "\n",
    "        if len(self.minute_buffer) >= 30:\n",
    "            self._process_30minute_candle()\n",
    "            self.minute_buffer.clear()\n",
    "\n",
    "    def add_candle(self, candle: Dict):\n",
    "        interval = candle.get('interval', '1m')\n",
    "\n",
    "        if interval == '30m':\n",
    "            self.add_30minute_candle(candle)\n",
    "        else:\n",
    "            self.add_1minute_candle(candle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## `add_30minute_candle`\n",
    "\n",
    "Used for bootstrapped or live 30m candles. Converts the candle dict to a Spark Row in order to match DataFrame row, appends it to the 30m deque (FIFO approach, first in first out), logs, and triggers prediction when the window has exactly 21 candles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_30minute_candle(self, candle: Dict):\n",
    "        from pyspark.sql import Row\n",
    "\n",
    "        candle_row = Row(\n",
    "            open_time=candle['open_time'],\n",
    "            symbol=candle['symbol'],\n",
    "            open=candle['open'],\n",
    "            high=candle['high'],\n",
    "            low=candle['low'],\n",
    "            close=candle['close'],\n",
    "            volume=candle['volume'],\n",
    "            number_of_trades=candle['number_of_trades'],\n",
    "            taker_buy_quote_asset_volume=candle['taker_buy_quote_asset_volume']\n",
    "        )\n",
    "\n",
    "        self.window_30m.append(candle_row)\n",
    "\n",
    "        logger.info(\n",
    "            f\"Added 30m candle: {candle['symbol']} \"\n",
    "            f\"@ {candle['open_time']} | Close: {candle['close']:.2f} \"\n",
    "            f\"(Window: {len(self.window_30m)}/{WINDOW_SIZE})\"\n",
    "        )\n",
    "\n",
    "        if len(self.window_30m) == WINDOW_SIZE:\n",
    "            self._predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## `_process_30minute_candle`\n",
    "\n",
    "Builds a DataFrame from the 1m buffer, aggregates to 30m using `aggregate_candles`, takes the single resulting row, appends it to the window, and runs prediction if the window is full. Used only when 1m candles are fed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _process_30minute_candle(self):\n",
    "        df_1min = self.spark.createDataFrame(self.minute_buffer)\n",
    "        df_30m = aggregate_candles(df_1min)\n",
    "        new_30m_row = df_30m.collect()[0]\n",
    "        self.window_30m.append(new_30m_row)\n",
    "\n",
    "        logger.info(\n",
    "            f\"Aggregated 30m candle: {new_30m_row['symbol']} \"\n",
    "            f\"@ {new_30m_row['open_time']} | Close: {new_30m_row['close']:.2f}\"\n",
    "        )\n",
    "\n",
    "        if len(self.window_30m) == WINDOW_SIZE:\n",
    "            self._predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## `_predict` — Core prediction pipeline\n",
    "\n",
    "Turns the 21-candle window into a DataFrame and gets the latest candle. If there is a `pending_prediction`, it backfills that row’s actual label using the current close. Then it generates features with `generate_label=False` (no future data). If no rows survive feature generation, it returns. Otherwise it runs RF, GBT, and LR, adds `rf_prob`, `gbt_prob`, `lr_prob` via `extract_prob_udf`, joins these by `open_time`, assembles meta-features, runs the meta-model, and gets `final_prob` and `prediction`. It computes the trading signal, prints it, logs the prediction to CSV, and stores this prediction in `pending_prediction` for the next backfill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _predict(self):\n",
    "        df_window = self.spark.createDataFrame(list(self.window_30m))\n",
    "        current_candle = self.window_30m[-1]\n",
    "\n",
    "        if self.pending_prediction is not None:\n",
    "            self._backfill_actual_label(\n",
    "                self.pending_prediction['timestamp'],\n",
    "                self.pending_prediction['close'],\n",
    "                float(current_candle['close'])\n",
    "            )\n",
    "\n",
    "        df_features, _ = generate_features(\n",
    "            df_window,\n",
    "            dataset_name=\"LIVE_STREAM\",\n",
    "            generate_label=False\n",
    "        )\n",
    "\n",
    "        if df_features.count() == 0:\n",
    "            logger.warning(\"No features generated - skipping prediction\")\n",
    "            return\n",
    "\n",
    "        rf_res = self.rf_model.transform(df_features).withColumn(\n",
    "            \"rf_prob\", extract_prob_udf(col(\"probability\"))\n",
    "        )\n",
    "        gbt_res = self.gbt_model.transform(df_features).withColumn(\n",
    "            \"gbt_prob\", extract_prob_udf(col(\"probability\"))\n",
    "        )\n",
    "        lr_res = self.lr_model.transform(df_features).withColumn(\n",
    "            \"lr_prob\", extract_prob_udf(col(\"probability\"))\n",
    "        )\n",
    "\n",
    "        meta_df = (rf_res.select(\"open_time\", \"symbol\", \"close\", \"rf_prob\")\n",
    "            .join(gbt_res.select(\"open_time\", \"gbt_prob\"), \"open_time\")\n",
    "            .join(lr_res.select(\"open_time\", \"lr_prob\"), \"open_time\"))\n",
    "\n",
    "        meta_assembler = VectorAssembler(\n",
    "            inputCols=[\"rf_prob\", \"gbt_prob\", \"lr_prob\"],\n",
    "            outputCol=\"meta_features\"\n",
    "        )\n",
    "        meta_df = meta_assembler.transform(meta_df)\n",
    "\n",
    "        final_prediction = self.meta_model.transform(meta_df)\n",
    "        final_prediction = final_prediction.withColumn(\n",
    "            \"final_prob\", extract_prob_udf(col(\"probability\"))\n",
    "        )\n",
    "\n",
    "        result = final_prediction.collect()[0]\n",
    "        prob_up = float(result['final_prob'])\n",
    "        prediction = float(result['prediction'])\n",
    "\n",
    "        signal, confidence = self._determine_signal(prob_up, prediction)\n",
    "\n",
    "        self._print_trade_signal(result['symbol'], signal, prob_up, confidence)\n",
    "\n",
    "        self._log_prediction(result, prob_up, prediction, current_candle)\n",
    "\n",
    "        logger.info(\n",
    "            f\"Prediction: {result['symbol']} | Prob UP: {prob_up:.4f} | \"\n",
    "            f\"Signal: {signal} | Confidence: {confidence}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## `_backfill_actual_label`\n",
    "\n",
    "When a new candle arrives, we know whether the previous candle closed up or down. This method sets `actual_label` (1 if current close > previous close, else 0), compares it to `pending_prediction['prediction']`, and writes `actual_label` and `is_correct` into the appropriate row of `data/predictions.csv` . In the end reads the whole file, updates that row, and writes it back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _backfill_actual_label(self, prev_timestamp: int, prev_close: float, current_close: float):\n",
    "        import csv\n",
    "\n",
    "        try:\n",
    "            actual_label = 1 if current_close > prev_close else 0\n",
    "\n",
    "            predicted = self.pending_prediction.get('prediction', None)\n",
    "            row_num = self.pending_prediction.get('row_number', None)\n",
    "\n",
    "            if predicted is not None and row_num is not None:\n",
    "                is_correct = 1 if predicted == actual_label else 0\n",
    "\n",
    "                predicted_str = \"UP\" if predicted == 1 else \"DOWN\"\n",
    "                actual_str = \"UP\" if actual_label == 1 else \"DOWN\"\n",
    "\n",
    "                if is_correct == 1:\n",
    "                    logger.info(f\"Previous prediction correct: Predicted {predicted_str}, Actual {actual_str}\")\n",
    "                else:\n",
    "                    logger.info(f\"Previous prediction wrong: Predicted {predicted_str}, Actual {actual_str}\")\n",
    "\n",
    "                log_file = \"data/predictions.csv\"\n",
    "\n",
    "                with open(log_file, 'r') as f:\n",
    "                    rows = list(csv.reader(f))\n",
    "\n",
    "                if row_num < len(rows):\n",
    "                    rows[row_num][6] = str(actual_label)\n",
    "                    rows[row_num][7] = str(is_correct)\n",
    "\n",
    "                with open(log_file, 'w', newline='') as f:\n",
    "                    writer = csv.writer(f)\n",
    "                    writer.writerows(rows)\n",
    "\n",
    "                logger.info(f\"Backfilled row {row_num} in CSV\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error backfilling: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## `_determine_signal`\n",
    "\n",
    "Maps the meta-model’s UP probability and predicted class to a human-readable signal and confidence. prob ≥ 0.7 or ≤ 0.3 → VERY HIGH (BUY or SELL); 0.6/0.4 → HIGH; otherwise Low confidence and WAIT. BUY/SELL corresponds (1 = Long, 0 = Short)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _determine_signal(self, prob_up: float, prediction: float) -> tuple:\n",
    "        if prob_up >= 0.7 or prob_up <= 0.3:\n",
    "            confidence = \"VERY HIGH\"\n",
    "            signal = \"BUY (Long)\" if prediction == 1.0 else \"SELL (Short)\"\n",
    "        elif prob_up >= 0.6 or prob_up <= 0.4:\n",
    "            confidence = \"HIGH\"\n",
    "            signal = \"BUY (Long)\" if prediction == 1.0 else \"SELL (Short)\"\n",
    "        else:\n",
    "            confidence = \"Low\"\n",
    "            signal = \"WAIT\"\n",
    "\n",
    "        return signal, confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## `_log_prediction` and `_count_csv_rows`\n",
    "\n",
    "`_log_prediction` ensures `data/predictions.csv` exists. It appends one row per prediction: timestamp, datetime string, symbol, close, final_prob, prediction; actual_label and is_correct are left empty and filled later by `_backfill_actual_label`. It then sets `pending_prediction` with timestamp, close, prediction, and the CSV data row index (from `_count_csv_rows`) so the next run can update that row. `_count_csv_rows` returns the number of data rows without header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _log_prediction(self, result, prob_up: float, prediction: float, current_candle):\n",
    "        import csv\n",
    "        from datetime import datetime\n",
    "\n",
    "        log_file = \"data/predictions.csv\"\n",
    "\n",
    "        if not os.path.exists(log_file):\n",
    "            os.makedirs(\"data\", exist_ok=True)\n",
    "            with open(log_file, 'w', newline='') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow([\n",
    "                    'timestamp', 'datetime', 'symbol', 'close',\n",
    "                    'final_prob', 'prediction', 'actual_label', 'is_correct'\n",
    "                ])\n",
    "\n",
    "        timestamp = result['open_time']\n",
    "        dt_str = datetime.fromtimestamp(timestamp / 1000).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        with open(log_file, 'a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([\n",
    "                timestamp,\n",
    "                dt_str,\n",
    "                result['symbol'],\n",
    "                float(result['close']),\n",
    "                float(prob_up),\n",
    "                int(prediction),\n",
    "                '',\n",
    "                ''\n",
    "            ])\n",
    "\n",
    "        logger.info(f\"Logged prediction to {log_file}\")\n",
    "\n",
    "        self.pending_prediction = {\n",
    "            'timestamp': result['open_time'],\n",
    "            'close': float(current_candle['close']),\n",
    "            'prediction': int(prediction),\n",
    "            'row_number': self._count_csv_rows(log_file)\n",
    "        }\n",
    "\n",
    "    def _count_csv_rows(self, filepath):\n",
    "        with open(filepath, 'r') as f:\n",
    "            return sum(1 for line in f) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## `_print_trade_signal`\n",
    "\n",
    "Prints the symbol, action (BUY/SELL/WAIT), probability, and confidence. Uses ANSI escape codes: green for BUY, red for SELL, yellow for WAIT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _print_trade_signal(self, symbol: str, signal: str, prob: float, confidence: str):\n",
    "        if \"BUY\" in signal:\n",
    "            color = \"\\033[92m\"\n",
    "        elif \"SELL\" in signal:\n",
    "            color = \"\\033[91m\"\n",
    "        else:\n",
    "            color = \"\\033[93m\"\n",
    "        reset = \"\\033[0m\"\n",
    "\n",
    "        print(f\"Signal: {symbol}\")\n",
    "        print(f\"Action: {color}{signal}{reset}\")\n",
    "        print(f\"Probability: {prob:.4f}\")\n",
    "        print(f\"Confidence Level: {confidence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## `run_stack_inference`\n",
    "\n",
    "Creates a single `CandlesProcessor` and defines a `batch_function` function that, for each micro-batch of the streaming DataFrame, collects rows and passes each candle as dict type to `processor.add_candle`. The streaming query writes to this batch function, uses the given checkpoint directory, and triggers every 10 seconds. Returns the started query so the caller can await termination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_stack_inference(parsed_streaming_df):\n",
    "    processor = CandlesProcessor()\n",
    "\n",
    "    def batch_function(batch_df, batch_id):\n",
    "        if batch_df.count() == 0:\n",
    "            return\n",
    "\n",
    "        records = batch_df.collect()\n",
    "        for row in records:\n",
    "            processor.add_candle(row.asDict())\n",
    "\n",
    "    query = (parsed_streaming_df.writeStream\n",
    "        .foreachBatch(batch_function)\n",
    "        .option(\"checkpointLocation\", \"./binance_kline_chk\")\n",
    "        .trigger(processingTime=\"10 seconds\")\n",
    "        .start())\n",
    "\n",
    "    logger.info(\"Stream started, waiting for candles\")\n",
    "    logger.info(\"Note: First prediction will have no backfilled prediction\")\n",
    "    logger.info(\"Subsequent predictions will show if previous prediction was correct\")\n",
    "    return query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## `if __name__ == \"__main__\"` — Standalone execution\n",
    "\n",
    "When the file is run manualy directly, it builds a Spark session, defines the schema for the Kafka JSON payload, reads from the `binance_kline` topic latest offset, parses the value with `from_json`, and runs `run_stack_inference` on the parsed stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.sql.types import StructType, StructField, StringType, DoubleType, LongType, IntegerType\n",
    "\n",
    "    spark = SparkSession.builder.appName(\"CryptoSignals\").getOrCreate()\n",
    "\n",
    "    schema = StructType([\n",
    "        StructField(\"open_time\", LongType(), True),\n",
    "        StructField(\"open\", DoubleType(), True),\n",
    "        StructField(\"high\", DoubleType(), True),\n",
    "        StructField(\"low\", DoubleType(), True),\n",
    "        StructField(\"close\", DoubleType(), True),\n",
    "        StructField(\"volume\", DoubleType(), True),\n",
    "        StructField(\"quote_asset_volume\", DoubleType(), True),\n",
    "        StructField(\"number_of_trades\", IntegerType(), True),\n",
    "        StructField(\"taker_buy_base_asset_volume\", DoubleType(), True),\n",
    "        StructField(\"taker_buy_quote_asset_volume\", DoubleType(), True),\n",
    "        StructField(\"symbol\", StringType(), True),\n",
    "        StructField(\"interval\", StringType(), True),\n",
    "        StructField(\"ingested_at\", StringType(), True)\n",
    "    ])\n",
    "\n",
    "    df_stream = (spark.readStream\n",
    "        .format(\"kafka\")\n",
    "        .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\n",
    "        .option(\"subscribe\", \"binance_kline\")\n",
    "        .option(\"startingOffsets\", \"earliest\")\n",
    "        .load())\n",
    "\n",
    "    from pyspark.sql.functions import from_json\n",
    "\n",
    "    df_parsed = df_stream.select(\n",
    "        from_json(col(\"value\").cast(\"string\"), schema).alias(\"data\")\n",
    "    ).select(\"data.*\")\n",
    "\n",
    "    query = run_stack_inference(df_parsed)\n",
    "\n",
    "    try:\n",
    "        query.awaitTermination()\n",
    "    except KeyboardInterrupt:\n",
    "        query.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
