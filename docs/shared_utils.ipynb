{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shared utils \n",
    "\n",
    "This document explains each section of `src/shared_utils.py`. The module provides functions used by both training semi-final.ipynb and live-predictions real_time_inference.py: candle aggregation, feature generation, and a UDF to extract the UP-class probability from Spark ML output.\n",
    "\n",
    "## Imports\n",
    "\n",
    "PySpark SQL (Window, column functions, types), ML feature (VectorAssembler), and reduce for building expressions.  \n",
    "col, lag, lead, when, unix_timestamp, floor, lit, first, last, max as _max, min as _min, sum as _sum, pow, sqrt, avg, abs as sabs, greatest, udf are aliased to avoid shadowing Python built-ins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import (\n",
    "    col, lag, lead, when, unix_timestamp, floor, lit,\n",
    "    first, last, max as _max, min as _min, sum as _sum,\n",
    "    pow, sqrt, avg, abs as sabs, greatest, udf\n",
    ")\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `extract_prob_udf`\n",
    "\n",
    "Spark ML classification models output a probability vector; for binary classification the second element is the probability of the positive class. This UDF takes that vector and returns that value as a Double, or 0.5 if the vector is null. Used in inference to get rf_prob, gbt_prob, lr_prob, and final_prob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_prob_udf = udf(lambda v: float(v[1]) if v is not None else 0.5, DoubleType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `aggregate_candles` — Timestamp handling\n",
    "\n",
    "Aggregates smaller-timeframe candles (1m) into larger buckets (30m). First we need a single time representation: training data often has open_time as TimestampType, while streaming sends unix milliseconds. So we check the column type and add open_time_ms either by converting the timestamp or by using the column itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_candles(df, candle_minutes=30):\n",
    "    from pyspark.sql.types import TimestampType\n",
    "\n",
    "    open_time_type = df.schema[\"open_time\"].dataType\n",
    "\n",
    "    if isinstance(open_time_type, TimestampType):\n",
    "        df = df.withColumn(\"open_time_ms\", unix_timestamp(col(\"open_time\")) * 1000)\n",
    "    else:\n",
    "        df = df.withColumn(\"open_time_ms\", col(\"open_time\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## `aggregate_candles` — Bucketing and aggregation\n",
    "\n",
    "Time is bucketed by integer division by the interval length in milliseconds, then multiplied back so the bucket is the interval start. We decided to group this bucket by symbol and aggregate: first open, max high, min low, last close, and sums for volume, number_of_trades, and taker_buy_quote_asset_volume. The result is ordered by symbol and open_time. The column is renamed back to open_time for downstream use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    df = df.withColumn(\n",
    "            \"time_bucket\",\n",
    "            floor(col(\"open_time_ms\") / lit(candle_minutes * 60 * 1000))\n",
    "            * lit(candle_minutes * 60 * 1000)\n",
    "        )\n",
    "\n",
    "        df = df.groupBy(\"symbol\", \"time_bucket\").agg(\n",
    "            first(\"open\").alias(\"open\"),\n",
    "            _max(\"high\").alias(\"high\"),\n",
    "            _min(\"low\").alias(\"low\"),\n",
    "            last(\"close\").alias(\"close\"),\n",
    "            _sum(\"volume\").alias(\"volume\"),\n",
    "            _sum(\"number_of_trades\").alias(\"number_of_trades\"),\n",
    "            _sum(\"taker_buy_quote_asset_volume\").alias(\"taker_buy_quote_asset_volume\")\n",
    "        ).withColumnRenamed(\"time_bucket\", \"open_time\").orderBy(\"symbol\", \"open_time\")\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `generate_features` — Windows and lag config\n",
    "\n",
    "Feature-generation implies two window specs partition by symbol and order by open_time. Lag configuration specifies which columns get which lags: e.g. high lags 1–6, close lags 1–lookback, open 3/5/6, number_of_trades and taker_buy_quote_asset_volume 1–3. These lags are the base for many derived features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features(df, dataset_name=\"\", lookback=20, generate_label=True):\n",
    "    print(f\"Start feature generation for {dataset_name}\")\n",
    "\n",
    "    window_spec = Window.partitionBy(\"symbol\").orderBy(\"open_time\")\n",
    "    window_symbol = Window.partitionBy(\"symbol\").orderBy(\"open_time\")\n",
    "\n",
    "    lag_config = {\n",
    "        \"high\": [1, 2, 3, 4, 5, 6],\n",
    "        \"close\": list(range(1, lookback + 1)),\n",
    "        \"open\": [3, 5, 6],\n",
    "        \"number_of_trades\": [1, 2, 3],\n",
    "        \"taker_buy_quote_asset_volume\": [1, 2, 3]\n",
    "    }\n",
    "\n",
    "    for column, lags in lag_config.items():\n",
    "        for lag_period in lags:\n",
    "            df = df.withColumn(\n",
    "                f\"{column}_lag{lag_period}\",\n",
    "                lag(col(column), lag_period).over(window_spec)\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## `generate_features` — Label generation\n",
    "\n",
    "For training we need a label: UP (1) if next close > current close, DOWN (0) otherwise. We use lead(close, 1) to get the next close, then assign the label and drop the previously created helper column. For inference we don’t have future data, so we add a dummy label (0); the model expects a label column but it’s not used for prediction. This was done to avoid data leakage during streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if generate_label:\n",
    "        df = df.withColumn(\"close_next\", lead(col(\"close\"), 1).over(window_spec))\n",
    "        df = df.withColumn(\"label\",\n",
    "            when(col(\"close_next\") > col(\"close\"), 1)\n",
    "            .when(col(\"close_next\") < col(\"close\"), 0)\n",
    "            .otherwise(0)\n",
    "        )\n",
    "        df = df.drop(\"close_next\")\n",
    "    else:\n",
    "        df = df.withColumn(\"label\", lit(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## `generate_features` — Candlestick and SMA features\n",
    "\n",
    "Adds candlestick shape: \n",
    "body (close − open), \n",
    "range (high − low), \n",
    "upper_wick and lower_wick. \n",
    "Then SMA 5/10/20 from the corresponding close lags,\n",
    "price deviation from each SMA as ratios (price_to_sma5/10/20). \n",
    "The raw SMA columns are dropped after the ratio is computed to keep the schema small !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"body\", col(\"close\") - col(\"open\"))\n",
    "    df = df.withColumn(\"range\", col(\"high\") - col(\"low\"))\n",
    "    df = df.withColumn(\n",
    "        \"upper_wick\",\n",
    "        col(\"high\") - when(col(\"close\") > col(\"open\"), col(\"close\")).otherwise(col(\"open\"))\n",
    "    )\n",
    "    df = df.withColumn(\n",
    "        \"lower_wick\",\n",
    "        when(col(\"close\") < col(\"open\"), col(\"close\")).otherwise(col(\"open\")) - col(\"low\")\n",
    "    )\n",
    "\n",
    "    df = df.withColumn(\"sma_5\",\n",
    "        (col(\"close_lag1\") + col(\"close_lag2\") + col(\"close_lag3\") + col(\"close_lag4\") + col(\"close_lag5\")) / 5)\n",
    "    df = df.withColumn(\"price_to_sma5\",\n",
    "        when(col(\"sma_5\") != 0, (col(\"close\") - col(\"sma_5\")) / col(\"sma_5\")).otherwise(0))\n",
    "    df = df.drop(\"sma_5\")\n",
    "\n",
    "    close_lags_10 = [col(f\"close_lag{i}\") for i in range(1, 11)]\n",
    "    df = df.withColumn(\"sma_10\", reduce(lambda a, b: a + b, close_lags_10) / lit(10))\n",
    "    df = df.withColumn(\"price_to_sma10\",\n",
    "        when(col(\"sma_10\") != 0, (col(\"close\") - col(\"sma_10\")) / col(\"sma_10\")).otherwise(0)\n",
    "    )\n",
    "    df = df.drop(\"sma_10\")\n",
    "\n",
    "    close_lags_20 = [col(f\"close_lag{i}\") for i in range(1, 21)]\n",
    "    df = df.withColumn(\"sma_20\", reduce(lambda a, b: a + b, close_lags_20) / lit(20))\n",
    "    df = df.withColumn(\"price_to_sma20\",\n",
    "        when(col(\"sma_20\") != 0, (col(\"close\") - col(\"sma_20\")) / col(\"sma_20\")).otherwise(0)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## `generate_features` — Momentum, volatility, Bollinger, ATR (More complex features that showed best importance rates during trainings)\n",
    "\n",
    "5-period price momentum (relative change from close_lag5)\n",
    "simple volatility measure from the last three close returns, \n",
    "Bollinger-style position as (close − sma_20) / (2 × volatility)\n",
    "    volatility and sma_20 are dropped after !\n",
    "True range is max(high−low, |high−prev_close|, |low−prev_close|)\n",
    "ATR is the average over the previous 10 rows (rowsBetween(-10, -1))\n",
    "    true_range is dropped after atr_10 is computed !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"price_momentum\",\n",
    "        when(col(\"close_lag5\") != 0, (col(\"close\") - col(\"close_lag5\")) / col(\"close_lag5\"))\n",
    "        .otherwise(0)\n",
    "    )\n",
    "\n",
    "    df = df.withColumn(\"volatility\",\n",
    "        sqrt((\n",
    "            pow((col(\"close_lag1\") - col(\"close_lag2\")) / col(\"close_lag2\"), 2) +\n",
    "            pow((col(\"close_lag2\") - col(\"close_lag3\")) / col(\"close_lag3\"), 2) +\n",
    "            pow((col(\"close_lag3\") - col(\"close_lag4\")) / col(\"close_lag4\"), 2)\n",
    "        ) / 3)\n",
    "    )\n",
    "\n",
    "    df = df.withColumn(\"bb_position\",\n",
    "        when(col(\"volatility\") != 0,\n",
    "            (col(\"close\") - col(\"sma_20\")) / (2 * col(\"volatility\"))\n",
    "        ).otherwise(0)\n",
    "    )\n",
    "    df = df.drop(\"volatility\", \"sma_20\")\n",
    "\n",
    "    df = df.withColumn(\"true_range\",\n",
    "        greatest(\n",
    "            col(\"high\") - col(\"low\"),\n",
    "            sabs(col(\"high\") - lag(col(\"close\"), 1).over(window_symbol)),\n",
    "            sabs(col(\"low\") - lag(col(\"close\"), 1).over(window_symbol))\n",
    "        )\n",
    "    )\n",
    "    window_tr = Window.partitionBy(\"symbol\").orderBy(\"open_time\").rowsBetween(-10, -1)\n",
    "    df = df.withColumn(\"atr_10\", avg(col(\"true_range\")).over(window_tr))\n",
    "    df = df.drop(\"true_range\")\n",
    "\n",
    "    df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## `generate_features` — Feature selection and vector assembly\n",
    "\n",
    "Only a subset of the computed columns are used by the model. \n",
    "SELECTED_FEATURES lists them: price_to_sma5/10/20, price_momentum, body/range/wicks, selected lags for high/close/open/trades/taker volume, bb_position, atr_10. \n",
    "We keep these plus system columns (symbol, open_time, label, close), drop the rest to save memory, then use VectorAssembler with handleInvalid=\"skip\" to build the feature vector. \n",
    "The result DataFrame has symbol, features, label, open_time, and close. \n",
    "The function returns this DataFrame and the SELECTED_FEATURES list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECTED_FEATURES = [\n",
    "        \"price_to_sma5\", \"price_to_sma10\", \"price_to_sma20\",\n",
    "        \"price_momentum\",\n",
    "        \"body\", \"range\", \"upper_wick\", \"lower_wick\",\n",
    "        \"high_lag1\", \"high_lag2\", \"high_lag3\", \"high_lag4\", \"high_lag5\", \"high_lag6\",\n",
    "        \"close_lag3\", \"close_lag4\",\n",
    "        \"open_lag3\", \"open_lag5\", \"open_lag6\",\n",
    "        \"number_of_trades_lag1\", \"number_of_trades_lag2\", \"number_of_trades_lag3\",\n",
    "        \"taker_buy_quote_asset_volume_lag1\",\n",
    "        \"taker_buy_quote_asset_volume_lag2\",\n",
    "        \"taker_buy_quote_asset_volume_lag3\",\n",
    "        \"bb_position\", \"atr_10\"\n",
    "    ]\n",
    "\n",
    "    system_cols = [\"symbol\", \"open_time\", \"label\", \"close\"]\n",
    "    keep_cols = set(SELECTED_FEATURES + system_cols)\n",
    "    drop_cols = [c for c in df.columns if c not in keep_cols]\n",
    "    if drop_cols:\n",
    "        df = df.drop(*drop_cols)\n",
    "\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=SELECTED_FEATURES,\n",
    "        outputCol=\"features\",\n",
    "        handleInvalid=\"skip\"\n",
    "    )\n",
    "    df = assembler.transform(df)\n",
    "\n",
    "    result_df = df.select(\"symbol\", \"features\", \"label\", \"open_time\", \"close\")\n",
    "\n",
    "    return result_df, SELECTED_FEATURES"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
